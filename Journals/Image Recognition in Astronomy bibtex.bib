
@article{bektesevic_linear_2017,
	title = {Linear feature detection algorithm for astronomical surveys – {I}. {Algorithm} description},
	volume = {471},
	issn = {00358711},
	doi = {10.1093/mnras/stx1565},
	abstract = {Computer vision algorithms are powerful tools in astronomical image analyses, especially when automation of object detection and extraction is required. Modern object detection algorithms in astronomy are oriented towards detection of stars and galaxies, ignoring completely the detection of existing linear features. With the emergence of wide-field sky surveys, linear features attract scientific interest as possible trails of fast flybys of near-Earth asteroids and meteors. In this work, we describe a new linear feature detection algorithm designed specifically for implementation in big data astronomy. The algorithm combines a series of algorithmic steps that first remove other objects (stars and galaxies) from the image and then enhance the line to enable more efficient line detection with the Hough algorithm. The rate of false positives is greatly reduced thanks to a step that replaces possible line segments with rectangles and then compares lines fitted to the rectangles with the lines obtained directly from the image. The speed of the algorithm and its applicability in astronomical surveys are also discussed.},
	number = {3},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Bektešević, Dino and Vinković, Dejan},
	month = nov,
	year = {2017},
	keywords = {STARS, GALAXIES, COMPUTER vision, surveys, asteroids: general, ASTRONOMICAL surveys, COMPUTER algorithms, DATA extraction, meteors, methods: data analysis, minor planets, NEAR-earth asteroids},
	pages = {2626--2641},
	file = {Submitted Version:C\:\\Users\\annab\\Zotero\\storage\\PWNAEDS6\\Bektešević and Vinković - 2017 - Linear feature detection algorithm for astronomica.pdf:application/pdf},
}

@article{li_tracking_2014,
	title = {Tracking the {Trajectory} of {Space} {Debris} in {Close} {Proximity} via a {Vision}-{Based} {Method}},
	volume = {27},
	issn = {08931321},
	url = {http://ezproxy.lib.utexas.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94449454&site=ehost-live},
	doi = {10.1061/(ASCE)AS.1943-5525.0000265},
	abstract = {The increasingly cluttered environment in space is placing a premium on future spacecraft and satellites that are capable of tracking and estimating the trajectory of unknown space debris autonomously without consistent communication with ground stations. In this paper, a vision-based debris trajectory-tracking method in close proximity is presented using two cameras onboard of two satellites in a formation. To differentiate the target debris from other clutters, a data-association technique is investigated. A two-stage nonlinear robust controller is developed to adjust the attitude of the satellites such that the target debris can be maintained within the field of view of the onboard cameras. Capabilities of the proposed estimation and control methods are validated in the simulations.},
	number = {2},
	urldate = {2022-08-09},
	journal = {Journal of Aerospace Engineering},
	author = {Li, N. and Xu, Y. and Basset, G. and Fitz-Coy, N. G.},
	month = apr,
	year = {2014},
	note = {Publisher: American Society of Civil Engineers},
	keywords = {COMPUTER vision, Aerospace engineering, ARTIFICIAL satellites, Astronomy, Control systems, Data association, Debris, Monitoring, Robust control, Simulation, Space debris, SPACE debris, SPACE vehicles, TRAJECTORIES (Mechanics), Vision-based estimation},
	pages = {238--248},
	file = {EBSCO Full Text:C\:\\Users\\annab\\Zotero\\storage\\N5RJHNAA\\Li et al. - 2014 - Tracking the Trajectory of Space Debris in Close P.pdf:application/pdf},
}

@article{auddy_dpnnet-20_2021,
	title = {{DPNNet}-2.0. {I}. {Finding} {Hidden} {Planets} from {Simulated} {Images} of {Protoplanetary} {Disk} {Gaps}},
	volume = {920},
	issn = {0004637X},
	doi = {10.3847/1538-4357/ac1518},
	abstract = {The observed substructures, like annular gaps, in dust emissions from protoplanetary disks are often interpreted as signatures of embedded planets. Fitting a model of planetary gaps to these observed features using customized simulations or empirical relations can reveal the characteristics of the hidden planets. However, customized fitting is often impractical owing to the increasing sample size and the complexity of diskâ€"planet interaction. In this paper we introduce the architecture of DPNNet-2.0, second in the series after DPNNet, designed using a convolutional neural network (CNN, specifically ResNet50 here) for predicting exoplanet masses directly from simulated images of protoplanetary disks hosting a single planet. DPNNet-2.0 additionally consists of a multi-input framework that uses both a CNN and multilayer perceptron (a class of artificial neural network) for processing image and disk parameters simultaneously. This enables DPNNet-2.0 to be trained using images directly, with the added option of considering disk parameters (disk viscosities, disk temperatures, disk surface-density profiles, dust abundances, and particle Stokes numbers) generated from diskâ€"planet hydrodynamic simulations as inputs. This work provides the required framework and is the first step toward the use of computer vision (implementing CNNs) to directly extract the mass of an exoplanet from planetary gaps observed in dust surface-density maps by telescopes such as the Atacama Large Millimeter/submillimeter Array.},
	number = {1},
	journal = {Astrophysical Journal},
	author = {Auddy, Sayantan and Dey, Ramit and Lin, Min-Kai and Hall, Cassandra},
	month = oct,
	year = {2021},
	keywords = {COMPUTER vision, CONVOLUTIONAL neural networks, ARTIFICIAL neural networks, OPTICAL disks, PLANETARY systems, PLANETS},
	pages = {1--12},
	file = {Submitted Version:C\:\\Users\\annab\\Zotero\\storage\\JKBN7NT3\\Auddy et al. - 2021 - DPNNet-2.0. I. Finding Hidden Planets from Simulat.pdf:application/pdf},
}

@article{civilini_detecting_2021,
	title = {Detecting moonquakes using convolutional neural networks, a non-local training set, and transfer learning},
	volume = {225},
	issn = {0956540X},
	doi = {10.1093/gji/ggab083},
	abstract = {The costly power requirements of delivering seismic data back to Earth from planetary missions requires the development of algorithms for lander-side signal analysis for telemetry prioritization. This is difficult to explicitly program, especially if no prior seismic data are available from the planetary body. Deep learning computer vision has been used to generalize seismic signals on Earth for earthquake early warning problems but such techniques have not yet been expanded to planetary science. We demonstrate that Convolutional Neural Networks can be used to accurately catalogue planetary seismicity without local training data by building binary noise/signal classifiers from a single Earth seismic station and applying the models to moonquakes from the Apollo Passive Seismic Experiment (PSE) and the Lunar Seismic Profiling Experiment (LSPE). In order to promote generality and reduce the amount of training data, the algorithms use spectral images instead of time-series. Two- to five-layer convolution models are tested against a subset of 200 Grade-A events from the PSE and obtained station accuracy averages of 89–96 per cent. As the model was applied to an hour trace of data (30 min before and after the Grade-A event), additional detections besides the Grade-A event are unavoidable. In order to comprehensively address algorithm accuracy, additional seismic detections corresponding to valid signals such as other moonquakes or multiples within a particularly long event needed to be compared with those caused by algorithm error or instrument glitches. We developed an 'extra-arrival accuracy' metric to quantify how many of the additional detections were due to valid seismic events and used it to select the three-layer model as the best fit. The three-layer model was applied to the entire LSPE record and matched the lunar day–night cycle driving thermal moonquake generation with fewer false detections than a recent study using Hidden Markov Models. We anticipate that these methods for lander-side signal detection can be easily expanded to non-seismological data and may provide even stronger results when supplemented with synthetic training data.},
	number = {3},
	journal = {Geophysical Journal International},
	author = {Civilini, F and Weber, R C and Jiang, Z and Phillips, D and Pan, W David},
	month = jun,
	year = {2021},
	keywords = {COMPUTER vision, CONVOLUTIONAL neural networks, DEEP learning, Earthquake monitoring and test-ban treaty verification, fuzzy logic, HIDDEN Markov models, LEARNING, LUNAR phases, Neural networks, Seismicity and tectonics},
	pages = {2120--2134},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\7DIMU6WH\\Civilini et al. - 2021 - Detecting moonquakes using convolutional neural ne.pdf:application/pdf},
}

@article{zhou_novel_2020,
	title = {A novel image classification model based on adversarial training for pulsar candidate identification},
	volume = {39},
	issn = {10641246},
	url = {http://ezproxy.lib.utexas.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147183983&site=ehost-live},
	doi = {10.3233/JIFS-200925},
	abstract = {Pulsars are highly magnetized, rotating neutron stars with small volume and high density. The discovery of pulsars is of great significance in the fields of physics and astronomy. With the development of artificial intelligent, image recognition models based on deep learning are increasingly utilized for pulsar candidate identification. However, pulsar candidate datasets are characterized by unbalance and lack of positive samples, which has contributed the traditional methods to fall into poor performance and model bias. To this end, a general image recognition model based on adversarial training is proposed. A generator, a classifier, and two discriminators are included in the model. Theoretical analysis demonstrates that the model has a unique optimal solution, and the classifier happens to be the inference network of the generator. Therefore, the samples produced by the generator significantly augment the diversity of training data. When the model reaches equilibrium, it can not only predict labels for unseen data, but also generate controllable samples. In experiments, we split part of data from MNIST for training. The results reveal that the model not only behaves better classification performance than CNN, but also has better controllability than CGAN and ACGAN. Then, the model is applied to pulsar candidate dataset HTRU and FAST. The results exhibit that, compared with CNN model, the F-score has increased by 1.99\% and 3.67\%, and the Recall has also increased by 6.28\% and 8.59\% respectively.},
	number = {5},
	urldate = {2022-08-09},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Zhou, Linyong and You, Shanping and Ren, Bimo and Yu, Xuhong and Xie, Xiaoyao},
	month = nov,
	year = {2020},
	note = {Publisher: IOS Press},
	keywords = {CONVOLUTIONAL neural networks, DEEP learning, IMAGE recognition (Computer vision), convolutional neural network, Generative adversarial nets, GEODETIC astronomy, pulsar candidate identification, PULSARS, unbalanced dataset},
	pages = {7657--7669},
	file = {EBSCO Full Text:C\:\\Users\\annab\\Zotero\\storage\\3JIZX94Z\\Zhou et al. - 2020 - A novel image classification model based on advers.pdf:application/pdf},
}

@article{he_deep_2021,
	title = {Deep learning applications based on {SDSS} photometric data: detection and classification of sources},
	volume = {508},
	issn = {00358711},
	shorttitle = {Deep learning applications based on {SDSS} photometric data},
	doi = {10.1093/mnras/stab2243},
	abstract = {Most astronomical source classification algorithms based on photometric data struggle to classify sources as quasars, stars, and galaxies reliably. To achieve this goal and build a new Sloan Digital Sky Survey photometric catalogue in the future, we apply a deep learning source detection network built on YOLO v4 object detection framework to detect sources and design a new deep learning classification network named APSCnet (astronomy photometric source classification network) to classify sources. In addition, a photometric background image generation network is applied to generate background images in the process of data sets synthesis. Our detection network obtains a mean average precision score of 88.02 when IOU = 0.5. As for APSCnet, in a magnitude range with 14–25, we achieve a precision of 84.1 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 93.2 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for quasars, a precision of 94.5 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 84.6 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for stars, and a precision of 95.8 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 95.1 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for galaxies; and in a magnitude range with less than 20, we achieve a precision of 96.6 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 94.7 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for quasars, a precision of 95.7 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 97.4 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for stars, and a precision of 98.9 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 99.2 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for galaxies. We have proved the superiority of our algorithm in the classification of astronomical sources through comparative experiments between multiple sets of methods. In addition, we also analysed the impact of point spread function on the classification results. These technologies may be applied to data mining of the next generation sky surveys, such as LSST, WFIRST, and CSST etc.},
	number = {2},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {He, Zhendong and Qiu, Bo and Luo, A-Li and Shi, Jinghang and Kong, Xiao and Jiang, Xia},
	month = dec,
	year = {2021},
	keywords = {DEEP learning, QUASARS, ASTRONOMICAL surveys, methods: data analysis, catalogues, CLASSIFICATION algorithms, DATA mining, GALACTIC magnitudes, galaxies: general, OBJECT recognition (Computer vision), SLOAN Digital Sky Survey, stars: general, techniques: image processing},
	pages = {2039--2052},
}

@article{macaluso_pulling_2018,
	title = {Pulling out all the tops with computer vision and deep learning},
	volume = {2018},
	issn = {11266708},
	doi = {10.1007/JHEP10(2018)121},
	abstract = {We apply computer vision with deep learning — in the form of a convolutional neural network (CNN) — to build a highly effective boosted top tagger. Previous work (the “DeepTop” tagger of Kasieczka et al) has shown that a CNN-based top tagger can achieve comparable performance to state-of-the-art conventional top taggers based on high-level inputs. Here, we introduce a number of improvements to the DeepTop tagger, including architecture, training, image preprocessing, sample size and color pixels. Our final CNN top tagger outperforms BDTs based on high-level inputs by a factor of ∼ 2-3 or more in background rejection, over a wide range of tagging efficiencies and fiducial jet selections. As reference points, we achieve a QCD background rejection factor of 500 (60) at 50\% top tagging efficiency for fully-merged (non-merged) top jets with pT in the 800-900 GeV (350-450 GeV) range. Our CNN can also be straightforwardly extended to the classification of other types of jets, and the lessons learned here may be useful to others designing their own deep NNs for LHC applications.},
	number = {10},
	journal = {Journal of High Energy Physics},
	author = {Macaluso, Sebastian and Shih, David},
	month = oct,
	year = {2018},
	keywords = {COMPUTER vision, ARTIFICIAL neural networks, DEEP learning, JETS (Nuclear physics), Jets, LARGE Hadron Collider, QUANTUM chromodynamics},
	pages = {1--1},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\WL4U7KKF\\Macaluso and Shih - 2018 - Pulling out all the tops with computer vision and .pdf:application/pdf},
}

@article{lipsa_visualization_2012,
	title = {Visualization for the {Physical} {Sciences}},
	volume = {31},
	issn = {01677055},
	url = {http://ezproxy.lib.utexas.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=82763841&site=ehost-live},
	doi = {10.1111/j.1467-8659.2012.03184.x},
	abstract = {Close collaboration with other scientific fields is an important goal for the visualization community. Yet engaging in a scientific collaboration can be challenging. The physical sciences, namely astronomy, chemistry, earth sciences and physics, exhibit an extensive range of research directions, providing exciting challenges for visualization scientists and creating ample possibilities for collaboration. We present the first survey of its kind that provides a comprehensive view of existing work on visualization for the physical sciences. We introduce novel classification schemes based on application area, data dimensionality and main challenge addressed, and apply these classifications to each contribution from the literature. Our survey helps in understanding the status of current research and serves as a useful starting point for those interested in visualization for the physical sciences.},
	number = {8},
	urldate = {2022-08-09},
	journal = {Computer Graphics Forum},
	author = {Lipşa, Dan R. and Laramee, Robert S. and Cox, Simon J. and Roberts, Jonathan C. and Walker, Rick and Borkin, Michelle A. and Pfister, Hanspeter},
	month = dec,
	year = {2012},
	note = {Publisher: Wiley-Blackwell},
	keywords = {COMPUTER vision, MATHEMATICAL models, CLASSIFICATION, COMPUTER graphics, COMPUTER simulation, I.3.8 [Computer Graphics]: Applications-Visualization, I.6.6 [Simulation and Modeling]: Symulation Output Analysis-Visualization, J.2 [Physical Sciences and Engineering]: Astronomy-Visualization, J.2 [Physical Sciences and Engineering]: Chemistry-Visualization, J.2 [Physical Sciences and Engineering]: Earth and atmospheric sciences-Visualization, J.2 [Physical Sciences and Engineering]: Physics-Visualization, PHYSICAL sciences, scientific visualization, visualization},
	pages = {2317--2347},
	file = {EBSCO Full Text:C\:\\Users\\annab\\Zotero\\storage\\KXJYK44D\\Lipşa et al. - 2012 - Visualization for the Physical Sciences.pdf:application/pdf},
}

@article{noauthor_radio_2018,
	title = {Radio {Galaxy} {Zoo}: machine learning for radio source host galaxy cross-identification},
	volume = {478},
	issn = {00358711},
	shorttitle = {Radio {Galaxy} {Zoo}},
	doi = {10.1093/mnras/sty1308},
	abstract = {We consider the problem of determining the host galaxies of radio sources by cross-identification. This has traditionally been done manually, which will be intractable for wide-area radio surveys like the Evolutionary Map of the Universe. Automated cross-identification will be critical for these future surveys, and machine learning may provide the tools to develop such methods. We apply a standard approach from computer vision to cross-identification, introducing one possible way of automating this problem, and explore the pros and cons of this approach. We apply our method to the 1.4 GHz Australian Telescope Large Area Survey (ATLAS) observations of the Chandra Deep Field South (CDFS) and the ESO Large Area ISO Survey South 1 fields by cross-identifying them with the Spitzer Wide-area Infrared Extragalactic survey. We train our method with two sets of data: expert cross-identifications of CDFS from the initial ATLAS data release and crowdsourced cross-identifications of CDFS from Radio Galaxy Zoo. We found that a simple strategy of cross-identifying a radio component with the nearest galaxy performs comparably to our more complex methods, though our estimated best-case performance is near 100 per cent. ATLAS contains 87 complex radio sources that have been cross-identified by experts, so there are not enough complex examples to learn how to cross-identify them accurately. Much larger data sets are therefore required for training methods like ours. We also show that training our method on Radio Galaxy Zoo cross-identifications gives comparable results to training on expert cross-identifications, demonstrating the value of crowdsourced training data.},
	number = {4},
	journal = {Monthly Notices of the Royal Astronomical Society},
	month = aug,
	year = {2018},
	keywords = {ASTRONOMY, ASTRONOMICAL observations, GALAXIES, ARTIFICIAL intelligence, MACHINE learning, galaxies: active, infrared: galaxies, methods: statistical, radio continuum: galaxies, techniques: miscellaneous},
	pages = {5547--5563},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\JBHCZBBQ\\2018 - Radio Galaxy Zoo machine learning for radio sourc.pdf:application/pdf},
}

@article{christiansen_gaussian_2020,
	title = {Gaussian representation for image recognition and reinforcement learning of atomistic structure},
	volume = {153},
	issn = {00219606},
	doi = {10.1063/5.0015571},
	abstract = {The success of applying machine learning to speed up structure search and improve property prediction in computational chemical physics depends critically on the representation chosen for the atomistic structure. In this work, we investigate how different image representations of two planar atomistic structures (ideal graphene and graphene with a grain boundary region) influence the ability of a reinforcement learning algorithm [the Atomistic Structure Learning Algorithm (ASLA)] to identify the structures from no prior knowledge while interacting with an electronic structure program. Compared to a one-hot encoding, we find a radial Gaussian broadening of the atomic position to be beneficial for the reinforcement learning process, which may even identify the Gaussians with the most favorable broadening hyperparameters during the structural search. Providing further image representations with angular information inspired by the smooth overlap of atomic positions method, however, is not found to cause further speedup of ASLA.},
	number = {4},
	journal = {Journal of Chemical Physics},
	author = {Christiansen, Mads-Peter V. and Mortensen, Henrik Lund and Meldgaard, Søren Ager and Hammer, Bjørk},
	month = jul,
	year = {2020},
	keywords = {MACHINE learning, IMAGE recognition (Computer vision), LEARNING, COMPUTATIONAL physics, IMAGE representation, REINFORCEMENT learning},
	pages = {1--11},
}

@article{bird_advances_2021,
	title = {Advances in deep space exploration via simulators \& deep learning},
	volume = {84},
	issn = {13841076},
	doi = {10.1016/j.newast.2020.101517},
	abstract = {• General classification models are not in-depth enough for deep space exploration. • Simulated images can provide a plethora of neural network training. • Simulated images can train neural networks to identify real exoplanets as well as real images would, or better. • Novelty detection, combined with simulated images, is turned into simple object detection. • Simulator images are the basis for an entirely more accurate predictive ai spacecraft. The NASA Starlight and Breakthrough Starshot programs conceptualize fast interstellar travel via small relativistic spacecraft that are propelled by directed energy. This process is radically different from traditional space travel and trades large and slow spacecraft for small, fast, inexpensive, and fragile ones. The main goal of these wafer satellites is to gather useful images during their deep space journey. We introduce and solve some of the main problems that accompany this concept. First, we need an object detection system that can detect planets that we have never seen before, some containing features that we may not even know exist in the universe. Second, once we have images of exoplanets, we need a way to take these images and rank them by importance. Equipment fails and data rates are slow, thus we need a method to ensure that the most important images to humankind are the ones that are prioritized for data transfer. Finally, the energy on board is minimal and must be conserved and used sparingly. No exoplanet images should be missed, but using energy erroneously would be detrimental. We introduce simulator-based methods that leverage artificial intelligence, mostly in the form of computer vision, in order to solve all three of these issues. Our results confirm that simulators provide an extremely rich training environment that surpasses that of real images, and can be used to train models on features that have yet to be observed by humans. We also show that the immersive and adaptable environment provided by the simulator, combined with deep learning, lets us navigate and save energy in an otherwise implausible way.},
	journal = {New Astronomy},
	author = {Bird, James and Petzold, Linda and Lubin, Philip and Deacon, Julia},
	month = apr,
	year = {2021},
	keywords = {COMPUTER vision, ARTIFICIAL intelligence, Computer vision, Deep learning, ENERGY consumption, Exoplanet, EXTRASOLAR planets, MICROSPACECRAFT, Novelty detection, Object detection, Simulator, Space, SPACE exploration, SPACE flight, Universe},
	pages = {N.PAG--N.PAG},
	file = {Accepted Version:C\:\\Users\\annab\\Zotero\\storage\\9NVDQYG5\\Bird et al. - 2021 - Advances in deep space exploration via simulators .pdf:application/pdf},
}

@article{mong_machine_2020,
	title = {Machine learning for transient recognition in difference imaging with minimum sampling effort},
	volume = {499},
	issn = {00358711},
	doi = {10.1093/mnras/staa3096},
	abstract = {The amount of observational data produced by time-domain astronomy is exponentially increasing. Human inspection alone is not an effective way to identify genuine transients from the data. An automatic real-bogus classifier is needed and machine learning techniques are commonly used to achieve this goal. Building a training set with a sufficiently large number of verified transients is challenging, due to the requirement of human verification. We present an approach for creating a training set by using all detections in the science images to be the sample of real detections and all detections in the difference images, which are generated by the process of difference imaging to detect transients, to be the samples of bogus detections. This strategy effectively minimizes the labour involved in the data labelling for supervised machine learning methods. We demonstrate the utility of the training set by using it to train several classifiers utilizing as the feature representation the normalized pixel values in 21 × 21 pixel stamps centred at the detection position, observed with the Gravitational-wave Optical Transient Observer (GOTO) prototype. The real-bogus classifier trained with this strategy can provide up to {\textbar}\$95\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} prediction accuracy on the real detections at a false alarm rate of {\textbar}\$1\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar}⁠.},
	number = {4},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Mong, Y-L and Ackley, K and Galloway, D K and Killestein, T and Lyman, J and Steeghs, D and Dhillon, V and O'Brien, P T and Ramsay, G and Poshyachinda, S and Kotak, R and Nuttall, L and Pallé, E and Pollacco, D and Thrane, E and Dyer, M J and Ulaczyk, K and Cutter, R and McCormac, J and Chote, P},
	month = dec,
	year = {2020},
	keywords = {MACHINE learning, IMAGE processing, IMAGE recognition (Computer vision), methods: data analysis, techniques: image processing, methods: statistical, GOAL (Psychology), SUPERVISED learning},
	pages = {6009--6017},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\NVPS5HRQ\\Mong et al. - 2020 - Machine learning for transient recognition in diff.pdf:application/pdf},
}

@article{masias_quantitative_2013,
	title = {A quantitative analysis of source detection approaches in optical, infrared, and radio astronomical images},
	volume = {36},
	issn = {09226435},
	url = {http://ezproxy.lib.utexas.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=92774783&site=ehost-live},
	doi = {10.1007/s10686-013-9346-1},
	abstract = {A variety of software is used to solve the challenging task of detecting astronomical sources in wide field images. Additionally, computer vision methods based on well-known or innovative techniques are arising to face this purpose. In this paper, we review several of the most promising methods that have emerged during the last few years in the field of source detection. We specifically focus on methods that have been designed to deal with images with Gaussian noise distributions. The singularity of this analysis is that the different methods have been applied to a single dataset consisting of optical, infrared, and radio images. Thus, the different approaches are applied on a level playing field, and the results obtained can be used to evaluate and compare the methods in a meaningful, quantitative way. Moreover, we present the most important strengths and weaknesses of the methods for each type of image as well as an extensive discussion where the methods with best performances are highlighted.},
	number = {3},
	urldate = {2022-08-09},
	journal = {Experimental Astronomy},
	author = {Masias, M. and Peracaula, M. and Freixenet, J. and Lladó, X.},
	month = dec,
	year = {2013},
	note = {Publisher: Springer Nature},
	keywords = {DATA analysis, Data analysis, Image processing, IMAGING systems in astronomy, INFRARED imaging, NUCLEAR counters, OPTICAL images, Quantitative analysis, QUANTITATIVE research, RADIO astronomy, Source detection},
	pages = {591--629},
	file = {EBSCO Full Text:C\:\\Users\\annab\\Zotero\\storage\\D6UKYRIY\\Masias et al. - 2013 - A quantitative analysis of source detection approa.pdf:application/pdf},
}

@article{li_new_2018,
	title = {A {New} {Square} {Grid}-{Based} {Method} for {Identifying} {Solar} {Activities}},
	volume = {32},
	issn = {02180014},
	doi = {10.1142/S0218001418500477},
	abstract = {The FITS is widely used as a common file format in astronomy. However, how to detect an effective celestial event from large-scale astronomical images with complex background is a thorny issue. This paper proposes a square grid structure for target detection (GSTD) of solar activities, inncluding the separation of the target area and the deletion of the background area. This kind of processing can greatly reduce the storage cost of the data set, while the solar activity area can be well preserved. The solar image is operated through an equally sized square grid to separate the solar activity target area and the background area to speed up the processing of images, improve processing accuracy, and effectively prevent image noise interference. The experiment results have shown that this method delivers satisfactory performance in accuracy and time-cost. Through the anti-jamming processing of image noise, the accurate positioning and effective recognition of solar activities are realized. This method has been proved to achieve good image segmentation recognition of solar phenomenon in the research of various types of solar activities. Moreover, it can provide a feasible way to reduce the storage occupancy of Content-Based Image Retrieval (CBIR) systems.},
	number = {12},
	journal = {International Journal of Pattern Recognition \& Artificial Intelligence},
	author = {Li, Weijiang and Yi, Jing and Qi, Xin},
	month = dec,
	year = {2018},
	keywords = {IMAGE recognition (Computer vision), IMAGING systems in astronomy, CONTENT-based image retrieval, GSTD, image segmentation, IMAGE segmentation, multi-threshold, SDO, solar activities, SOLAR activity},
	pages = {N.PAG--N.PAG},
}

@article{baek_solar_2021,
	title = {Solar {Event} {Detection} {Using} {Deep}-{Learning}-{Based} {Object} {Detection} {Methods}},
	volume = {296},
	issn = {00380938},
	doi = {10.1007/s11207-021-01902-5},
	abstract = {Research on the detection of solar events has been conducted over many years. Recently, deep learning and data-driven approaches have been applied to solar event recognition. In this study, we present solar event detection using deep-learning-based object detection methods for real-time space weather monitoring. First, we construct a new object detection dataset using imaging data obtained by the Solar Dynamics Observatory with bounding boxes as labels for three representative features: coronal holes, sunspots, and prominences. Second, we train two representative object detection models: the Single Shot MultiBox Detector (SSD) and the Faster Region-based Convolutional Neural Network (R-CNN) using the new dataset. The results show that both models perform similarly well for coronal hole and sunspot detection. For prominence detection, the SSD and Faster R-CNN exhibited relatively low performance. This study demonstrates that deep-learning-based object detection can successfully detect multiple types of solar events, and it may be extended to detect other solar events. In addition, we provide the dataset for further achievements of object detection studies in solar physics.},
	number = {11},
	journal = {Solar Physics},
	author = {Baek, Ji-Hye and Kim, Sujin and Choi, Seonghwan and Park, Jongyeob and Kim, Jihun and Jo, Wonkeun and Kim, Dongil},
	month = nov,
	year = {2021},
	keywords = {SUN, CONVOLUTIONAL neural networks, DEEP learning, OBJECT recognition (Computer vision), Coronal holes, Data management, Event detection, Prominences, SPACE environment, Sunspots},
	pages = {1--15},
}

@article{albert_model-independent_2020,
	title = {Model-independent search for neutrino sources with the {ANTARES} neutrino telescope},
	volume = {114},
	issn = {09276505},
	doi = {10.1016/j.astropartphys.2019.06.003},
	abstract = {A novel method to analyse the spatial distribution of neutrino candidates recorded with the ANTARES neutrino telescope is introduced, searching for an excess of neutrinos in a region of arbitrary size and shape from any direction in the sky. Techniques originating from the domains of machine learning, pattern recognition and image processing are used to purify the sample of neutrino candidates and for the analysis of the obtained skymap. In contrast to a dedicated search for a specific neutrino emission model, this approach is sensitive to a wide range of possible morphologies of potential sources of high-energy neutrino emission. The application of these methods to ANTARES data yields a large-scale excess with a post-trial significance of 2.5 σ. Applied to public data from IceCube in its IC40 configuration, an excess consistent with the results from ANTARES is observed with a post-trial significance of 2.1 σ.},
	journal = {Astroparticle Physics},
	author = {Albert, A. and André, M. and Anghinolfi, M. and Anton, G. and Ardid, M. and Aubert, J.-J. and Avgitas, T. and Baret, B. and Barrios-Martí, J. and Basa, S. and Bertin, V. and Biagi, S. and Bormuth, R. and Bourret, S. and Bouwhuis, M.c. and Bruijn, R. and Brunner, J. and Busto, J. and Capone, A. and Caramete, L.},
	month = jan,
	year = {2020},
	keywords = {IMAGE processing, IMAGE recognition (Computer vision), TELESCOPES, Anisotropy, Astroparticle physics, Neutrino astronomy, NEUTRINO astrophysics, NEUTRINO detectors, NEUTRINOS, PATTERN perception, Pattern recognition, PHOTOMULTIPLIERS},
	pages = {35--47},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\5635UDIB\\Albert et al. - 2020 - Model-independent search for neutrino sources with.pdf:application/pdf},
}

@article{tenbrinck_image_2015,
	title = {Image segmentation with arbitrary noise models by solving minimal surface problems},
	volume = {48},
	issn = {00313203},
	doi = {10.1016/j.patcog.2015.01.006},
	abstract = {Segmentation is one of the fundamental tasks in computer vision applications. For natural images there exist a vast amount of sophisticated segmentation methods in the literature. However, these standard methods tend to fail in the presence of non-Gaussian image noise, e.g., in biomedical imaging or astronomy. In this paper we propose an adequate variational segmentation model for segmentation of images perturbed by arbitrary noise models without adding a-priori assumptions about the unknown physical noise model. For this, we discuss the prominent minimal surface problem and two different numerical minimization schemes to solve it. The first approach efficiently computes the set of all possible minimal surface solutions for the given data via convex optimization and reduces the segmentation problem to the estimation of a proper threshold. The second approach is based on level set methods and is especially suitable for the separation of inhomogeneous image regions. The advantage of this approach is both its simpleness and robustness: the noise in the data does not have to be modeled explicitly since the image intensities are separated using histogram-based thresholding techniques. The proposed model can be interpreted as a generalization of many traditional segmentation methods which implicitly assume a perturbation by additive Gaussian noise. The superiority of this approach over standard methods such as the popular Chan–Vese formulation is demonstrated on synthetic images as well as real application data from biomedical imaging.},
	number = {11},
	journal = {Pattern Recognition},
	author = {Tenbrinck, Daniel and Jiang, Xiaoyi},
	month = nov,
	year = {2015},
	keywords = {COMPUTER vision, IMAGE segmentation, Chan–Vese model, Convex optimization, IMAGE denoising, Level set methods, Minimal surface problem, Physical noise, PLATEAU'S problem, PROBLEM solving ability testing, Segmentation, Thresholding},
	pages = {3293--3309},
}

@article{zimmermann_deep_2019,
	title = {Deep neural networks for classifying complex features in diffraction images},
	volume = {99},
	issn = {24700045},
	doi = {10.1103/PhysRevE.99.063309},
	abstract = {Intense short-wavelength pulses from free-electron lasers and high-harmonic-generation sources enable diffractive imaging of individual nanosized objects with a single x-ray laser shot. The enormous data sets with up to several million diffraction patterns present a severe problem for data analysis because of the high dimensionality of imaging data. Feature recognition and selection is a crucial step to reduce the dimensionality. Usually, custom-made algorithms are developed at a considerable effort to approximate the particular features connected to an individual specimen, but because they face different experimental conditions, these approaches do not generalize well. On the other hand, deep neural networks are the principal instrument for today's revolution in automated image recognition, a development that has not been adapted to its full potential for data analysis in science. We recently published [Langbehn et al., Phys. Rev. Lett. 121, 255301 (2018)] the application of a deep neural network as a feature extractor for wide-angle diffraction images of helium nanodroplets. Here we present the setup, our modifications, and the training process of the deep neural network for diffraction image classification and its systematic bench marking. We find that deep neural networks significantly outperform previous attempts for sorting and classifying complex diffraction patterns and are a significant improvement for the much-needed assistance during postprocessing of large amounts of experimental coherent diffraction imaging data.},
	number = {6},
	journal = {Physical Review E},
	author = {Zimmermann, Julian and Langbehn, Bruno and Cucini, Riccardo and Di Fraia, Michele and Finetti, Paola and LaForge, Aaron C. and {Toshiyuki Nishiyama} and Ovcharenko, Yevheniy,  and Piseri, Paolo and Plekan, Oksana and Prince, Kevin C. and Stienkemeier, Frank and Ueda, Kiyoshi and Callegari, Carlo and Möller, Thomas and Rupp, Daniela},
	month = jun,
	year = {2019},
	keywords = {IMAGE recognition (Computer vision), DATA science, DIFFRACTION patterns, FEATURE selection, HARMONIC generation, X-ray lasers},
	pages = {1--1},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\EUIFG8J5\\Zimmermann et al. - 2019 - Deep neural networks for classifying complex featu.pdf:application/pdf},
}

@article{ribli_weak_2019,
	title = {Weak lensing cosmology with convolutional neural networks on noisy data},
	volume = {490},
	issn = {00358711},
	doi = {10.1093/mnras/stz2610},
	abstract = {Weak gravitational lensing is one of the most promising cosmological probes of the late universe. Several large ongoing (DES, KiDS, HSC) and planned (LSST, Euclid, WFIRST) astronomical surveys attempt to collect even deeper and larger scale data on weak lensing. Due to gravitational collapse, the distribution of dark matter is non-Gaussian on small scales. However, observations are typically evaluated through the two-point correlation function of galaxy shear, which does not capture non-Gaussian features of the lensing maps. Previous studies attempted to extract non-Gaussian information from weak lensing observations through several higher order statistics such as the three-point correlation function, peak counts, or Minkowski functionals. Deep convolutional neural networks (CNN) emerged in the field of computer vision with tremendous success, and they offer a new and very promising framework to extract information from 2D or 3D astronomical data sets, confirmed by recent studies on weak lensing. We show that a CNN is able to yield significantly stricter constraints of (σ8, Ωm) cosmological parameters than the power spectrum using convergence maps generated by full N -body simulations and ray-tracing, at angular scales and shape noise levels relevant for future observations. In a scenario mimicking LSST or Euclid , the CNN yields 2.4–2.8 times smaller credible contours than the power spectrum, and 3.5–4.2 times smaller at noise levels corresponding to a deep space survey such as WFIRST. We also show that at shape noise levels achievable in future space surveys the CNN yields 1.4–2.1 times smaller contours than peak counts, a higher order statistic capable of extracting non-Gaussian information from weak lensing maps.},
	number = {2},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Ribli, Dezső and Pataki, Bálint Ármin and Zorrilla Matilla, José Manuel and Hsu, Daniel and Haiman, Zoltán and Csabai, István},
	month = dec,
	year = {2019},
	keywords = {ARTIFICIAL neural networks, ASTRONOMICAL surveys, techniques: image processing, COSMIC background radiation, dark matter, DARK matter, EUCLID, FRIEDMANN equations, GRAVITATIONAL lenses, gravitational lensing: weak},
	pages = {1843--1860},
	file = {Submitted Version:C\:\\Users\\annab\\Zotero\\storage\\7IW8HLU6\\Ribli et al. - 2019 - Weak lensing cosmology with convolutional neural n.pdf:application/pdf},
}

@article{sabin_first_2021,
	title = {First deep images catalogue of extended {IPHAS} {PNe}},
	volume = {508},
	issn = {00358711},
	doi = {10.1093/mnras/stab2477},
	abstract = {We present the first instalment of a deep imaging catalogue containing 58 True, Likely, and Possible extended PNe detected with the Isaac Newton Telescope Photometric H α Survey (IPHAS). The three narrow-band filters in the emission lines of H α, [N  ii ] λ6584 Å, and [O  iii ] λ5007 Å used for this purpose allowed us to improve our description of the morphology and dimensions of the nebulae. In some cases even the nature of the source has been reassessed. We were then able to unveil new macro- and micro-structures, which will without a doubt contribute to a more accurate analysis of these PNe. It has been also possible to perform a primary classification of the targets based on their ionization level. A Deep Learning classification tool has also been tested. We expect that all the PNe from the IPHAS catalogue of new extended planetary nebulae will ultimately be part of this deep H α, [N  ii ], and [O  iii ] imaging catalogue.},
	number = {2},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Sabin, L and Guerrero, M A and Ramos-Larios, G and Boumis, P and Zijlstra, A A and Awang Iskandar, D N F and Barlow, M J and Toalá, J A and Parker, Q A and Corradi, R M L and Morris, R a H},
	month = dec,
	year = {2021},
	keywords = {NEBULAE, STELLAR winds, PLANETARY nebulae, DEEP learning, CATALOGING, CATALOGS, NEWTON, Isaac, 1642-1727, outflows, planetary nebulae, stars: mass-loss, stars: winds},
	pages = {1599--1617},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\AV4ALM9S\\Sabin et al. - 2021 - First deep images catalogue of extended IPHAS PNe.pdf:application/pdf},
}

@article{szabo_machine_2022,
	title = {Machine learning in present day astrophysics},
	volume = {53},
	issn = {05317479},
	doi = {10.1051/epn/2022205},
	abstract = {Machine learning is everywhere in our daily life. From the social media and bank sector to transportation and telecommunication, we cannot avoid using it, sometimes even without noticing that we are relying on it. Astronomy and astrophysics are no exception. From telescope time and survey telescope scheduling through object detection and classification, to cleaning images and making large simulations smarter and quicker to it is ubiquitous to use machine learning algorithms. To illustrate this silent revolution, we checked the NASA Astronomical Data System website and searched for the keyword 'machine learning' in abstracts of astronomical and astrophysical papers. In 2000 we found 56, in 2010 889, and by 2020 no less than 35,659 abstracts contained the magic two words.},
	number = {2},
	journal = {Europhysics News},
	author = {Szabó, R. and Szklenár, T. and Bódi, A.},
	month = may,
	year = {2022},
	keywords = {ASTRONOMY, ASTROPHYSICS, MACHINE learning, OBJECT recognition (Computer vision), KEYWORD searching, TELECOMMUNICATION, UNITED States. National Aeronautics \& Space Administration},
	pages = {22--25},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\7S5AEHLS\\Szabó et al. - 2022 - Machine learning in present day astrophysics.pdf:application/pdf},
}

@article{zhan-wei_space_2016,
	title = {Space {Object} {Tracking} {Method} {Based} on a {Snake} {Model}},
	volume = {40},
	issn = {02751062},
	doi = {10.1016/j.chinastron.2016.05.004},
	abstract = {In this paper, aiming at the problem of unstable tracking of low-orbit variable and bright space objects, adopting an active contour model, a kind of improved GVF (Gradient Vector Flow) - Snake algorithm is proposed to realize the real-time search of the real object contour on the CCD image. Combined with the Kalman filter for prediction, a new adaptive tracking method is proposed for space objects. Experiments show that this method can overcome the tracking error caused by the fixed window, and improve the tracking robustness.},
	number = {2},
	journal = {Chinese Astronomy \& Astrophysics},
	author = {Zhan-wei, Xu and Xin, Wang},
	month = apr,
	year = {2016},
	keywords = {COMPUTER vision, techniques: image processing, ARTIFICIAL satellite tracking, KALMAN filtering, MATHEMATICAL control theory, OBJECT tracking (Computer vision), space vehicles, telescopes},
	pages = {266--276},
}

@article{mohammadi_manifold_2022,
	title = {Manifold {Alignment} {Aware} {Ants}: {A} {Markovian} {Process} for {Manifold} {Extraction}},
	volume = {34},
	issn = {08997667},
	shorttitle = {Manifold {Alignment} {Aware} {Ants}},
	doi = {10.1162/neco_a_01478},
	abstract = {The presence of manifolds is a common assumption in many applications, including astronomy and computer vision. For instance, in astronomy, low-dimensional stellar structures, such as streams, shells, and globular clusters, can be found in the neighborhood of big galaxies such as the Milky Way. Since these structures are often buried in very large data sets, an algorithm, which can not only recover the manifold but also remove the background noise (or outliers), is highly desirable. While other works try to recover manifolds either by pushing all points toward manifolds or by downsampling from dense regions, aiming to solve one of the problems, they generally fail to suppress the noise on manifolds and remove background noise simultaneously. Inspired by the collective behavior of biological ants in food-seeking process, we propose a new algorithm that employs several random walkers equipped with a local alignment measure to detect and denoise manifolds. During the walking process, the agents release pheromone on data points, which reinforces future movements. Over time the pheromone concentrates on the manifolds, while it fades in the background noise due to an evaporation procedure. We use the Markov chain (MC) framework to provide a theoretical analysis of the convergence of the algorithm and its performance. Moreover, an empirical analysis, based on synthetic and real-world data sets, is provided to demonstrate its applicability in different areas, such as improving the performance of t -distributed stochastic neighbor embedding (t-SNE) and spectral clustering using the underlying MC formulas, recovering astronomical low-dimensional structures, and improving the performance of the fast Parzen window density estimator.},
	number = {3},
	journal = {Neural Computation},
	author = {Mohammadi, Mohammad and Tino, Peter and Bunte, Kerstin},
	month = mar,
	year = {2022},
	keywords = {MILKY Way, BIG data, ALGORITHM performance analysis, COLLECTIVE behavior, GLOBULAR clusters, STELLAR structure},
	pages = {595--641},
}

@article{hao_gradient-aligned_2022,
	title = {Gradient-{Aligned} convolution neural network},
	volume = {122},
	issn = {00313203},
	doi = {10.1016/j.patcog.2021.108354},
	abstract = {• We propose a general Convolution operation, called GAConv, which can replace conventional operations in CNN to help it achieve rotation invariance. • With GAConv, Gradient-Aligned CNN (GACNN) can achieve rotation invariance without any data augmentation, feature-map augmentation, and filter enrichment. • In GACNN, rotation invariance does not learn from the training set, but bases on the network model. Different from the vanilla CNN, GACNN will output invariant results for all rotated versions of an object, no matter whether the network is trained or not. • We conduct classification experiments on designed dataset and realistic datasets. The results show that with the same computation cost, GACNN achieved better results than conventional CNN and some rotational invariant CNN. Although Convolution Neural Networks (CNN) have achieved great success in many applications of computer vision in recent years, rotation invariance is still a difficult problem for CNN. Especially for some images, the content can appear in the image at any angle of rotation, such as medical images, microscopic images, remote sensing images and astronomical images. In this paper, we propose a novel convolution operation, called Gradient-Aligned Convolution (GAConv), which can help CNN achieve rotation invariance by replacing vanilla convolutions in CNN. GAConv is implemented with a prior pixel-level gradient alignment operation before regular convolution. With GAConv, Gradient-Aligned CNN (GACNN) can achieve rotation invariance without any data augmentation, feature-map augmentation, and filter enrichment. In GACNN, rotation invariance does not learn from the training set, but bases on the network model. Different from the vanilla CNN, GACNN will output invariant results for all rotated versions of an object, no matter whether the network is trained or not. This means that we only need to train the network with one canonical version of the object and all other rotated versions of this object should be recognized with the same accuracy. Classification experiments have been conducted to evaluate GACNN compared with some rotation invariant approaches. GACNN achieved the best results on the 360 ∘ rotated test set of MNIST-rotation, Plankton-sub-rotation, and Galaxy Zoo 2.},
	journal = {Pattern Recognition},
	author = {Hao, You and Hu, Ping and Li, Shirui and Udupa, Jayaram K. and Tong, Yubing and Li, Hua},
	month = feb,
	year = {2022},
	keywords = {COMPUTER vision, CONVOLUTIONAL neural networks, REMOTE sensing, DATA augmentation, Gradient alignment, Rotation equivariant convolution, Rotation invariant neural network},
	pages = {N.PAG--N.PAG},
}

@article{bom_deep_2021,
	title = {Deep {Learning} assessment of galaxy morphology in {S}-{PLUS} {Data} {Release} 1},
	volume = {507},
	issn = {00358711},
	doi = {10.1093/mnras/stab1981},
	abstract = {The morphological diversity of galaxies is a relevant probe of galaxy evolution and cosmological structure formation, but the classification of galaxies in large sky surveys is becoming a significant challenge. We use data from the Stripe-82 area observed by the Southern Photometric Local Universe Survey (S-PLUS) in 12 optical bands, and present a catalogue of the morphologies of galaxies brighter than r = 17 mag determined both using a novel multiband morphometric fitting technique and Convolutional Neural Networks (CNNs) for computer vision. Using the CNNs, we find that, compared to our baseline results with three bands, the performance increases when using 5 broad and 3 narrow bands, but is poorer when using the full 12 band S-PLUS image set. However, the best result is still achieved with just three optical bands when using pre-trained network weights from an ImageNet data set. These results demonstrate the importance of using prior knowledge about neural network weights based on training in unrelated, extensive data sets, when available. Our catalogue contains 3274 galaxies in Stripe-82 that are not present in Galaxy Zoo 1 (GZ1), and we also provide our classifications for 4686 galaxies that were considered ambiguous in GZ1. Finally, we present a prospect of a novel way to take advantage of 12 band information for morphological classification using morphometric features, and we release a model that has been pre-trained on several bands that could be adapted for classifications using data from other surveys. The morphological catalogues are publicly available.},
	number = {2},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Bom, C R and Cortesi, A and Lucatelli, G and Dias, L O and Schubert, P and Oliveira Schwarz, G B and Cardoso, N M and Lima, E V R and Mendes de Oliveira, C and Sodre, L and Smith Castelli, A V and Ferrari, F and Damke, G and Overzier, R and Kanaan, A and Ribeiro, T and Schoenell, W},
	month = oct,
	year = {2021},
	keywords = {CONVOLUTIONAL neural networks, DEEP learning, surveys, techniques: image processing, CLASSIFICATION of galaxies, GALACTIC evolution, galaxies: fundamental parameters, galaxies: structure, methods: miscellaneous, MORPHOLOGY},
	pages = {1937--1955},
	file = {Accepted Version:C\:\\Users\\annab\\Zotero\\storage\\X5DT6DFF\\Bom et al. - 2021 - Deep Learning assessment of galaxy morphology in S.pdf:application/pdf},
}

@article{banerjee_galaxy_2021,
	title = {Galaxy {Morphological} {Image} {Classification} using {ResNet}},
	volume = {62},
	issn = {00672904},
	doi = {10.24996/ijs.2021.62.10.27},
	abstract = {Machine learning-based techniques are used widely for the classification of images into various categories. The advancement of Convolutional Neural Network (CNN) affects the field of computer vision on a large scale. It has been applied to classify and localize objects in images. Among the fields of applications of CNN, it has been applied to understand huge unstructured astronomical data being collected every second. Galaxies have diverse and complex shapes and their morphology carries fundamental information about the whole universe. Studying these galaxies has been a tremendous task for the researchers around the world. Researchers have already applied some basic CNN models to predict the morphological classes of the galaxies. In this paper, a residual network (ResNet) model is applied for this purpose. The proposed methodology classified the galaxies depending on their shape into 37 different classes. The performance of the methodology was evaluated using the data set provided by Kaggle. In this data set, 61,578 galaxy images are given, which are classified by human eye. The model achieved nearly 98\% accuracy.},
	number = {10},
	journal = {Iraqi Journal of Science},
	author = {Banerjee, Siddhartha and Ghosh, Bibek Ranjan and Gangapadhyay, Ayan and Chatterjee, Himadri Sankar},
	month = oct,
	year = {2021},
	keywords = {GALAXIES, CONVOLUTIONAL neural networks, MACHINE learning, ASTRONOMICAL models, Convolutional Neural Network, Galaxy Morphological Classification, Galaxy Zoo Data set, Residual Networks, ResNet-18, UNIVERSE},
	pages = {3690--3696},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\LJFLZY4V\\Banerjee et al. - 2021 - Galaxy Morphological Image Classification using Re.pdf:application/pdf},
}

@article{shamir_automatic_2009,
	title = {Automatic morphological classification of galaxy images},
	volume = {399},
	issn = {00358711},
	doi = {10.1111/j.1365-2966.2009.15366.x},
	abstract = {We describe an image analysis supervised learning algorithm that can automatically classify galaxy images. The algorithm is first trained using manually classified images of elliptical, spiral and edge-on galaxies. A large set of image features is extracted from each image, and the most informative features are selected using Fisher scores. Test images can then be classified using a simple Weighted Nearest Neighbour rule such that the Fisher scores are used as the feature weights. Experimental results show that galaxy images from Galaxy Zoo can be classified automatically to spiral, elliptical and edge-on galaxies with an accuracy of ∼90 per cent compared to classifications carried out by the author. Full compilable source code of the algorithm is available for free download, and its general-purpose nature makes it suitable for other uses that involve automatic image analysis of celestial objects.},
	number = {3},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Shamir, Lior},
	month = nov,
	year = {2009},
	keywords = {ALGORITHMS, ASTRONOMY, GALAXIES, IMAGE analysis, MACHINE learning, methods: data analysis, techniques: image processing},
	pages = {1367--1372},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\CCQMMVEM\\Shamir - 2009 - Automatic morphological classification of galaxy i.pdf:application/pdf},
}
