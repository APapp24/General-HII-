
@article{bektesevic_linear_2017,
	title = {Linear feature detection algorithm for astronomical surveys – {I}. {Algorithm} description},
	volume = {471},
	issn = {00358711},
	doi = {10.1093/mnras/stx1565},
	abstract = {Computer vision algorithms are powerful tools in astronomical image analyses, especially when automation of object detection and extraction is required. Modern object detection algorithms in astronomy are oriented towards detection of stars and galaxies, ignoring completely the detection of existing linear features. With the emergence of wide-field sky surveys, linear features attract scientific interest as possible trails of fast flybys of near-Earth asteroids and meteors. In this work, we describe a new linear feature detection algorithm designed specifically for implementation in big data astronomy. The algorithm combines a series of algorithmic steps that first remove other objects (stars and galaxies) from the image and then enhance the line to enable more efficient line detection with the Hough algorithm. The rate of false positives is greatly reduced thanks to a step that replaces possible line segments with rectangles and then compares lines fitted to the rectangles with the lines obtained directly from the image. The speed of the algorithm and its applicability in astronomical surveys are also discussed.},
	number = {3},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Bektešević, Dino and Vinković, Dejan},
	month = nov,
	year = {2017},
	keywords = {STARS, GALAXIES, COMPUTER vision, surveys, asteroids: general, ASTRONOMICAL surveys, COMPUTER algorithms, DATA extraction, meteors, methods: data analysis, minor planets, NEAR-earth asteroids},
	pages = {2626--2641},
	file = {Submitted Version:C\:\\Users\\annab\\Zotero\\storage\\PWNAEDS6\\Bektešević and Vinković - 2017 - Linear feature detection algorithm for astronomica.pdf:application/pdf},
}

@article{li_tracking_2014,
	title = {Tracking the {Trajectory} of {Space} {Debris} in {Close} {Proximity} via a {Vision}-{Based} {Method}},
	volume = {27},
	issn = {08931321},
	url = {http://ezproxy.lib.utexas.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94449454&site=ehost-live},
	doi = {10.1061/(ASCE)AS.1943-5525.0000265},
	abstract = {The increasingly cluttered environment in space is placing a premium on future spacecraft and satellites that are capable of tracking and estimating the trajectory of unknown space debris autonomously without consistent communication with ground stations. In this paper, a vision-based debris trajectory-tracking method in close proximity is presented using two cameras onboard of two satellites in a formation. To differentiate the target debris from other clutters, a data-association technique is investigated. A two-stage nonlinear robust controller is developed to adjust the attitude of the satellites such that the target debris can be maintained within the field of view of the onboard cameras. Capabilities of the proposed estimation and control methods are validated in the simulations.},
	number = {2},
	urldate = {2022-08-09},
	journal = {Journal of Aerospace Engineering},
	author = {Li, N. and Xu, Y. and Basset, G. and Fitz-Coy, N. G.},
	month = apr,
	year = {2014},
	note = {Publisher: American Society of Civil Engineers},
	keywords = {COMPUTER vision, Aerospace engineering, ARTIFICIAL satellites, Astronomy, Control systems, Data association, Debris, Monitoring, Robust control, Simulation, Space debris, SPACE debris, SPACE vehicles, TRAJECTORIES (Mechanics), Vision-based estimation},
	pages = {238--248},
	file = {EBSCO Full Text:C\:\\Users\\annab\\Zotero\\storage\\N5RJHNAA\\Li et al. - 2014 - Tracking the Trajectory of Space Debris in Close P.pdf:application/pdf},
}

@article{auddy_dpnnet-20_2021,
	title = {{DPNNet}-2.0. {I}. {Finding} {Hidden} {Planets} from {Simulated} {Images} of {Protoplanetary} {Disk} {Gaps}},
	volume = {920},
	issn = {0004637X},
	doi = {10.3847/1538-4357/ac1518},
	abstract = {The observed substructures, like annular gaps, in dust emissions from protoplanetary disks are often interpreted as signatures of embedded planets. Fitting a model of planetary gaps to these observed features using customized simulations or empirical relations can reveal the characteristics of the hidden planets. However, customized fitting is often impractical owing to the increasing sample size and the complexity of diskâ€"planet interaction. In this paper we introduce the architecture of DPNNet-2.0, second in the series after DPNNet, designed using a convolutional neural network (CNN, specifically ResNet50 here) for predicting exoplanet masses directly from simulated images of protoplanetary disks hosting a single planet. DPNNet-2.0 additionally consists of a multi-input framework that uses both a CNN and multilayer perceptron (a class of artificial neural network) for processing image and disk parameters simultaneously. This enables DPNNet-2.0 to be trained using images directly, with the added option of considering disk parameters (disk viscosities, disk temperatures, disk surface-density profiles, dust abundances, and particle Stokes numbers) generated from diskâ€"planet hydrodynamic simulations as inputs. This work provides the required framework and is the first step toward the use of computer vision (implementing CNNs) to directly extract the mass of an exoplanet from planetary gaps observed in dust surface-density maps by telescopes such as the Atacama Large Millimeter/submillimeter Array.},
	number = {1},
	journal = {Astrophysical Journal},
	author = {Auddy, Sayantan and Dey, Ramit and Lin, Min-Kai and Hall, Cassandra},
	month = oct,
	year = {2021},
	keywords = {COMPUTER vision, CONVOLUTIONAL neural networks, ARTIFICIAL neural networks, OPTICAL disks, PLANETARY systems, PLANETS},
	pages = {1--12},
	file = {Submitted Version:C\:\\Users\\annab\\Zotero\\storage\\JKBN7NT3\\Auddy et al. - 2021 - DPNNet-2.0. I. Finding Hidden Planets from Simulat.pdf:application/pdf},
}

@article{civilini_detecting_2021,
	title = {Detecting moonquakes using convolutional neural networks, a non-local training set, and transfer learning},
	volume = {225},
	issn = {0956540X},
	doi = {10.1093/gji/ggab083},
	abstract = {The costly power requirements of delivering seismic data back to Earth from planetary missions requires the development of algorithms for lander-side signal analysis for telemetry prioritization. This is difficult to explicitly program, especially if no prior seismic data are available from the planetary body. Deep learning computer vision has been used to generalize seismic signals on Earth for earthquake early warning problems but such techniques have not yet been expanded to planetary science. We demonstrate that Convolutional Neural Networks can be used to accurately catalogue planetary seismicity without local training data by building binary noise/signal classifiers from a single Earth seismic station and applying the models to moonquakes from the Apollo Passive Seismic Experiment (PSE) and the Lunar Seismic Profiling Experiment (LSPE). In order to promote generality and reduce the amount of training data, the algorithms use spectral images instead of time-series. Two- to five-layer convolution models are tested against a subset of 200 Grade-A events from the PSE and obtained station accuracy averages of 89–96 per cent. As the model was applied to an hour trace of data (30 min before and after the Grade-A event), additional detections besides the Grade-A event are unavoidable. In order to comprehensively address algorithm accuracy, additional seismic detections corresponding to valid signals such as other moonquakes or multiples within a particularly long event needed to be compared with those caused by algorithm error or instrument glitches. We developed an 'extra-arrival accuracy' metric to quantify how many of the additional detections were due to valid seismic events and used it to select the three-layer model as the best fit. The three-layer model was applied to the entire LSPE record and matched the lunar day–night cycle driving thermal moonquake generation with fewer false detections than a recent study using Hidden Markov Models. We anticipate that these methods for lander-side signal detection can be easily expanded to non-seismological data and may provide even stronger results when supplemented with synthetic training data.},
	number = {3},
	journal = {Geophysical Journal International},
	author = {Civilini, F and Weber, R C and Jiang, Z and Phillips, D and Pan, W David},
	month = jun,
	year = {2021},
	keywords = {COMPUTER vision, CONVOLUTIONAL neural networks, DEEP learning, Earthquake monitoring and test-ban treaty verification, fuzzy logic, HIDDEN Markov models, LEARNING, LUNAR phases, Neural networks, Seismicity and tectonics},
	pages = {2120--2134},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\7DIMU6WH\\Civilini et al. - 2021 - Detecting moonquakes using convolutional neural ne.pdf:application/pdf},
}

@article{zhou_novel_2020,
	title = {A novel image classification model based on adversarial training for pulsar candidate identification},
	volume = {39},
	issn = {10641246},
	url = {http://ezproxy.lib.utexas.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147183983&site=ehost-live},
	doi = {10.3233/JIFS-200925},
	abstract = {Pulsars are highly magnetized, rotating neutron stars with small volume and high density. The discovery of pulsars is of great significance in the fields of physics and astronomy. With the development of artificial intelligent, image recognition models based on deep learning are increasingly utilized for pulsar candidate identification. However, pulsar candidate datasets are characterized by unbalance and lack of positive samples, which has contributed the traditional methods to fall into poor performance and model bias. To this end, a general image recognition model based on adversarial training is proposed. A generator, a classifier, and two discriminators are included in the model. Theoretical analysis demonstrates that the model has a unique optimal solution, and the classifier happens to be the inference network of the generator. Therefore, the samples produced by the generator significantly augment the diversity of training data. When the model reaches equilibrium, it can not only predict labels for unseen data, but also generate controllable samples. In experiments, we split part of data from MNIST for training. The results reveal that the model not only behaves better classification performance than CNN, but also has better controllability than CGAN and ACGAN. Then, the model is applied to pulsar candidate dataset HTRU and FAST. The results exhibit that, compared with CNN model, the F-score has increased by 1.99\% and 3.67\%, and the Recall has also increased by 6.28\% and 8.59\% respectively.},
	number = {5},
	urldate = {2022-08-09},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Zhou, Linyong and You, Shanping and Ren, Bimo and Yu, Xuhong and Xie, Xiaoyao},
	month = nov,
	year = {2020},
	note = {Publisher: IOS Press},
	keywords = {CONVOLUTIONAL neural networks, DEEP learning, IMAGE recognition (Computer vision), convolutional neural network, Generative adversarial nets, GEODETIC astronomy, pulsar candidate identification, PULSARS, unbalanced dataset},
	pages = {7657--7669},
	file = {EBSCO Full Text:C\:\\Users\\annab\\Zotero\\storage\\3JIZX94Z\\Zhou et al. - 2020 - A novel image classification model based on advers.pdf:application/pdf},
}

@article{he_deep_2021,
	title = {Deep learning applications based on {SDSS} photometric data: detection and classification of sources},
	volume = {508},
	issn = {00358711},
	shorttitle = {Deep learning applications based on {SDSS} photometric data},
	doi = {10.1093/mnras/stab2243},
	abstract = {Most astronomical source classification algorithms based on photometric data struggle to classify sources as quasars, stars, and galaxies reliably. To achieve this goal and build a new Sloan Digital Sky Survey photometric catalogue in the future, we apply a deep learning source detection network built on YOLO v4 object detection framework to detect sources and design a new deep learning classification network named APSCnet (astronomy photometric source classification network) to classify sources. In addition, a photometric background image generation network is applied to generate background images in the process of data sets synthesis. Our detection network obtains a mean average precision score of 88.02 when IOU = 0.5. As for APSCnet, in a magnitude range with 14–25, we achieve a precision of 84.1 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 93.2 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for quasars, a precision of 94.5 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 84.6 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for stars, and a precision of 95.8 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 95.1 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for galaxies; and in a magnitude range with less than 20, we achieve a precision of 96.6 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 94.7 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for quasars, a precision of 95.7 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 97.4 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for stars, and a precision of 98.9 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} at 99.2 {\textbar}\$\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} recall for galaxies. We have proved the superiority of our algorithm in the classification of astronomical sources through comparative experiments between multiple sets of methods. In addition, we also analysed the impact of point spread function on the classification results. These technologies may be applied to data mining of the next generation sky surveys, such as LSST, WFIRST, and CSST etc.},
	number = {2},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {He, Zhendong and Qiu, Bo and Luo, A-Li and Shi, Jinghang and Kong, Xiao and Jiang, Xia},
	month = dec,
	year = {2021},
	keywords = {DEEP learning, QUASARS, ASTRONOMICAL surveys, methods: data analysis, catalogues, CLASSIFICATION algorithms, DATA mining, GALACTIC magnitudes, galaxies: general, OBJECT recognition (Computer vision), SLOAN Digital Sky Survey, stars: general, techniques: image processing},
	pages = {2039--2052},
}

@article{macaluso_pulling_2018,
	title = {Pulling out all the tops with computer vision and deep learning},
	volume = {2018},
	issn = {11266708},
	doi = {10.1007/JHEP10(2018)121},
	abstract = {We apply computer vision with deep learning — in the form of a convolutional neural network (CNN) — to build a highly effective boosted top tagger. Previous work (the “DeepTop” tagger of Kasieczka et al) has shown that a CNN-based top tagger can achieve comparable performance to state-of-the-art conventional top taggers based on high-level inputs. Here, we introduce a number of improvements to the DeepTop tagger, including architecture, training, image preprocessing, sample size and color pixels. Our final CNN top tagger outperforms BDTs based on high-level inputs by a factor of ∼ 2-3 or more in background rejection, over a wide range of tagging efficiencies and fiducial jet selections. As reference points, we achieve a QCD background rejection factor of 500 (60) at 50\% top tagging efficiency for fully-merged (non-merged) top jets with pT in the 800-900 GeV (350-450 GeV) range. Our CNN can also be straightforwardly extended to the classification of other types of jets, and the lessons learned here may be useful to others designing their own deep NNs for LHC applications.},
	number = {10},
	journal = {Journal of High Energy Physics},
	author = {Macaluso, Sebastian and Shih, David},
	month = oct,
	year = {2018},
	keywords = {COMPUTER vision, ARTIFICIAL neural networks, DEEP learning, JETS (Nuclear physics), Jets, LARGE Hadron Collider, QUANTUM chromodynamics},
	pages = {1--1},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\WL4U7KKF\\Macaluso and Shih - 2018 - Pulling out all the tops with computer vision and .pdf:application/pdf},
}

@article{lipsa_visualization_2012,
	title = {Visualization for the {Physical} {Sciences}},
	volume = {31},
	issn = {01677055},
	url = {http://ezproxy.lib.utexas.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=82763841&site=ehost-live},
	doi = {10.1111/j.1467-8659.2012.03184.x},
	abstract = {Close collaboration with other scientific fields is an important goal for the visualization community. Yet engaging in a scientific collaboration can be challenging. The physical sciences, namely astronomy, chemistry, earth sciences and physics, exhibit an extensive range of research directions, providing exciting challenges for visualization scientists and creating ample possibilities for collaboration. We present the first survey of its kind that provides a comprehensive view of existing work on visualization for the physical sciences. We introduce novel classification schemes based on application area, data dimensionality and main challenge addressed, and apply these classifications to each contribution from the literature. Our survey helps in understanding the status of current research and serves as a useful starting point for those interested in visualization for the physical sciences.},
	number = {8},
	urldate = {2022-08-09},
	journal = {Computer Graphics Forum},
	author = {Lipşa, Dan R. and Laramee, Robert S. and Cox, Simon J. and Roberts, Jonathan C. and Walker, Rick and Borkin, Michelle A. and Pfister, Hanspeter},
	month = dec,
	year = {2012},
	note = {Publisher: Wiley-Blackwell},
	keywords = {COMPUTER vision, MATHEMATICAL models, CLASSIFICATION, COMPUTER graphics, COMPUTER simulation, I.3.8 [Computer Graphics]: Applications-Visualization, I.6.6 [Simulation and Modeling]: Symulation Output Analysis-Visualization, J.2 [Physical Sciences and Engineering]: Astronomy-Visualization, J.2 [Physical Sciences and Engineering]: Chemistry-Visualization, J.2 [Physical Sciences and Engineering]: Earth and atmospheric sciences-Visualization, J.2 [Physical Sciences and Engineering]: Physics-Visualization, PHYSICAL sciences, scientific visualization, visualization},
	pages = {2317--2347},
	file = {EBSCO Full Text:C\:\\Users\\annab\\Zotero\\storage\\KXJYK44D\\Lipşa et al. - 2012 - Visualization for the Physical Sciences.pdf:application/pdf},
}

@article{noauthor_radio_2018,
	title = {Radio {Galaxy} {Zoo}: machine learning for radio source host galaxy cross-identification},
	volume = {478},
	issn = {00358711},
	shorttitle = {Radio {Galaxy} {Zoo}},
	doi = {10.1093/mnras/sty1308},
	abstract = {We consider the problem of determining the host galaxies of radio sources by cross-identification. This has traditionally been done manually, which will be intractable for wide-area radio surveys like the Evolutionary Map of the Universe. Automated cross-identification will be critical for these future surveys, and machine learning may provide the tools to develop such methods. We apply a standard approach from computer vision to cross-identification, introducing one possible way of automating this problem, and explore the pros and cons of this approach. We apply our method to the 1.4 GHz Australian Telescope Large Area Survey (ATLAS) observations of the Chandra Deep Field South (CDFS) and the ESO Large Area ISO Survey South 1 fields by cross-identifying them with the Spitzer Wide-area Infrared Extragalactic survey. We train our method with two sets of data: expert cross-identifications of CDFS from the initial ATLAS data release and crowdsourced cross-identifications of CDFS from Radio Galaxy Zoo. We found that a simple strategy of cross-identifying a radio component with the nearest galaxy performs comparably to our more complex methods, though our estimated best-case performance is near 100 per cent. ATLAS contains 87 complex radio sources that have been cross-identified by experts, so there are not enough complex examples to learn how to cross-identify them accurately. Much larger data sets are therefore required for training methods like ours. We also show that training our method on Radio Galaxy Zoo cross-identifications gives comparable results to training on expert cross-identifications, demonstrating the value of crowdsourced training data.},
	number = {4},
	journal = {Monthly Notices of the Royal Astronomical Society},
	month = aug,
	year = {2018},
	keywords = {ASTRONOMY, ASTRONOMICAL observations, GALAXIES, ARTIFICIAL intelligence, MACHINE learning, galaxies: active, infrared: galaxies, methods: statistical, radio continuum: galaxies, techniques: miscellaneous},
	pages = {5547--5563},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\JBHCZBBQ\\2018 - Radio Galaxy Zoo machine learning for radio sourc.pdf:application/pdf},
}

@article{christiansen_gaussian_2020,
	title = {Gaussian representation for image recognition and reinforcement learning of atomistic structure},
	volume = {153},
	issn = {00219606},
	doi = {10.1063/5.0015571},
	abstract = {The success of applying machine learning to speed up structure search and improve property prediction in computational chemical physics depends critically on the representation chosen for the atomistic structure. In this work, we investigate how different image representations of two planar atomistic structures (ideal graphene and graphene with a grain boundary region) influence the ability of a reinforcement learning algorithm [the Atomistic Structure Learning Algorithm (ASLA)] to identify the structures from no prior knowledge while interacting with an electronic structure program. Compared to a one-hot encoding, we find a radial Gaussian broadening of the atomic position to be beneficial for the reinforcement learning process, which may even identify the Gaussians with the most favorable broadening hyperparameters during the structural search. Providing further image representations with angular information inspired by the smooth overlap of atomic positions method, however, is not found to cause further speedup of ASLA.},
	number = {4},
	journal = {Journal of Chemical Physics},
	author = {Christiansen, Mads-Peter V. and Mortensen, Henrik Lund and Meldgaard, Søren Ager and Hammer, Bjørk},
	month = jul,
	year = {2020},
	keywords = {MACHINE learning, IMAGE recognition (Computer vision), LEARNING, COMPUTATIONAL physics, IMAGE representation, REINFORCEMENT learning},
	pages = {1--11},
}

@article{bird_advances_2021,
	title = {Advances in deep space exploration via simulators \& deep learning},
	volume = {84},
	issn = {13841076},
	doi = {10.1016/j.newast.2020.101517},
	abstract = {• General classification models are not in-depth enough for deep space exploration. • Simulated images can provide a plethora of neural network training. • Simulated images can train neural networks to identify real exoplanets as well as real images would, or better. • Novelty detection, combined with simulated images, is turned into simple object detection. • Simulator images are the basis for an entirely more accurate predictive ai spacecraft. The NASA Starlight and Breakthrough Starshot programs conceptualize fast interstellar travel via small relativistic spacecraft that are propelled by directed energy. This process is radically different from traditional space travel and trades large and slow spacecraft for small, fast, inexpensive, and fragile ones. The main goal of these wafer satellites is to gather useful images during their deep space journey. We introduce and solve some of the main problems that accompany this concept. First, we need an object detection system that can detect planets that we have never seen before, some containing features that we may not even know exist in the universe. Second, once we have images of exoplanets, we need a way to take these images and rank them by importance. Equipment fails and data rates are slow, thus we need a method to ensure that the most important images to humankind are the ones that are prioritized for data transfer. Finally, the energy on board is minimal and must be conserved and used sparingly. No exoplanet images should be missed, but using energy erroneously would be detrimental. We introduce simulator-based methods that leverage artificial intelligence, mostly in the form of computer vision, in order to solve all three of these issues. Our results confirm that simulators provide an extremely rich training environment that surpasses that of real images, and can be used to train models on features that have yet to be observed by humans. We also show that the immersive and adaptable environment provided by the simulator, combined with deep learning, lets us navigate and save energy in an otherwise implausible way.},
	journal = {New Astronomy},
	author = {Bird, James and Petzold, Linda and Lubin, Philip and Deacon, Julia},
	month = apr,
	year = {2021},
	keywords = {COMPUTER vision, ARTIFICIAL intelligence, Computer vision, Deep learning, ENERGY consumption, Exoplanet, EXTRASOLAR planets, MICROSPACECRAFT, Novelty detection, Object detection, Simulator, Space, SPACE exploration, SPACE flight, Universe},
	pages = {N.PAG--N.PAG},
	file = {Accepted Version:C\:\\Users\\annab\\Zotero\\storage\\9NVDQYG5\\Bird et al. - 2021 - Advances in deep space exploration via simulators .pdf:application/pdf},
}

@article{mong_machine_2020,
	title = {Machine learning for transient recognition in difference imaging with minimum sampling effort},
	volume = {499},
	issn = {00358711},
	doi = {10.1093/mnras/staa3096},
	abstract = {The amount of observational data produced by time-domain astronomy is exponentially increasing. Human inspection alone is not an effective way to identify genuine transients from the data. An automatic real-bogus classifier is needed and machine learning techniques are commonly used to achieve this goal. Building a training set with a sufficiently large number of verified transients is challenging, due to the requirement of human verification. We present an approach for creating a training set by using all detections in the science images to be the sample of real detections and all detections in the difference images, which are generated by the process of difference imaging to detect transients, to be the samples of bogus detections. This strategy effectively minimizes the labour involved in the data labelling for supervised machine learning methods. We demonstrate the utility of the training set by using it to train several classifiers utilizing as the feature representation the normalized pixel values in 21 × 21 pixel stamps centred at the detection position, observed with the Gravitational-wave Optical Transient Observer (GOTO) prototype. The real-bogus classifier trained with this strategy can provide up to {\textbar}\$95\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar} prediction accuracy on the real detections at a false alarm rate of {\textbar}\$1\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\${\textbar}⁠.},
	number = {4},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Mong, Y-L and Ackley, K and Galloway, D K and Killestein, T and Lyman, J and Steeghs, D and Dhillon, V and O'Brien, P T and Ramsay, G and Poshyachinda, S and Kotak, R and Nuttall, L and Pallé, E and Pollacco, D and Thrane, E and Dyer, M J and Ulaczyk, K and Cutter, R and McCormac, J and Chote, P},
	month = dec,
	year = {2020},
	keywords = {MACHINE learning, IMAGE processing, IMAGE recognition (Computer vision), methods: data analysis, techniques: image processing, methods: statistical, GOAL (Psychology), SUPERVISED learning},
	pages = {6009--6017},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\NVPS5HRQ\\Mong et al. - 2020 - Machine learning for transient recognition in diff.pdf:application/pdf},
}

@article{masias_quantitative_2013,
	title = {A quantitative analysis of source detection approaches in optical, infrared, and radio astronomical images},
	volume = {36},
	issn = {09226435},
	url = {http://ezproxy.lib.utexas.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=92774783&site=ehost-live},
	doi = {10.1007/s10686-013-9346-1},
	abstract = {A variety of software is used to solve the challenging task of detecting astronomical sources in wide field images. Additionally, computer vision methods based on well-known or innovative techniques are arising to face this purpose. In this paper, we review several of the most promising methods that have emerged during the last few years in the field of source detection. We specifically focus on methods that have been designed to deal with images with Gaussian noise distributions. The singularity of this analysis is that the different methods have been applied to a single dataset consisting of optical, infrared, and radio images. Thus, the different approaches are applied on a level playing field, and the results obtained can be used to evaluate and compare the methods in a meaningful, quantitative way. Moreover, we present the most important strengths and weaknesses of the methods for each type of image as well as an extensive discussion where the methods with best performances are highlighted.},
	number = {3},
	urldate = {2022-08-09},
	journal = {Experimental Astronomy},
	author = {Masias, M. and Peracaula, M. and Freixenet, J. and Lladó, X.},
	month = dec,
	year = {2013},
	note = {Publisher: Springer Nature},
	keywords = {DATA analysis, Data analysis, Image processing, IMAGING systems in astronomy, INFRARED imaging, NUCLEAR counters, OPTICAL images, Quantitative analysis, QUANTITATIVE research, RADIO astronomy, Source detection},
	pages = {591--629},
	file = {EBSCO Full Text:C\:\\Users\\annab\\Zotero\\storage\\D6UKYRIY\\Masias et al. - 2013 - A quantitative analysis of source detection approa.pdf:application/pdf},
}

@article{li_new_2018,
	title = {A {New} {Square} {Grid}-{Based} {Method} for {Identifying} {Solar} {Activities}},
	volume = {32},
	issn = {02180014},
	doi = {10.1142/S0218001418500477},
	abstract = {The FITS is widely used as a common file format in astronomy. However, how to detect an effective celestial event from large-scale astronomical images with complex background is a thorny issue. This paper proposes a square grid structure for target detection (GSTD) of solar activities, inncluding the separation of the target area and the deletion of the background area. This kind of processing can greatly reduce the storage cost of the data set, while the solar activity area can be well preserved. The solar image is operated through an equally sized square grid to separate the solar activity target area and the background area to speed up the processing of images, improve processing accuracy, and effectively prevent image noise interference. The experiment results have shown that this method delivers satisfactory performance in accuracy and time-cost. Through the anti-jamming processing of image noise, the accurate positioning and effective recognition of solar activities are realized. This method has been proved to achieve good image segmentation recognition of solar phenomenon in the research of various types of solar activities. Moreover, it can provide a feasible way to reduce the storage occupancy of Content-Based Image Retrieval (CBIR) systems.},
	number = {12},
	journal = {International Journal of Pattern Recognition \& Artificial Intelligence},
	author = {Li, Weijiang and Yi, Jing and Qi, Xin},
	month = dec,
	year = {2018},
	keywords = {IMAGE recognition (Computer vision), IMAGING systems in astronomy, CONTENT-based image retrieval, GSTD, image segmentation, IMAGE segmentation, multi-threshold, SDO, solar activities, SOLAR activity},
	pages = {N.PAG--N.PAG},
}

@article{baek_solar_2021,
	title = {Solar {Event} {Detection} {Using} {Deep}-{Learning}-{Based} {Object} {Detection} {Methods}},
	volume = {296},
	issn = {00380938},
	doi = {10.1007/s11207-021-01902-5},
	abstract = {Research on the detection of solar events has been conducted over many years. Recently, deep learning and data-driven approaches have been applied to solar event recognition. In this study, we present solar event detection using deep-learning-based object detection methods for real-time space weather monitoring. First, we construct a new object detection dataset using imaging data obtained by the Solar Dynamics Observatory with bounding boxes as labels for three representative features: coronal holes, sunspots, and prominences. Second, we train two representative object detection models: the Single Shot MultiBox Detector (SSD) and the Faster Region-based Convolutional Neural Network (R-CNN) using the new dataset. The results show that both models perform similarly well for coronal hole and sunspot detection. For prominence detection, the SSD and Faster R-CNN exhibited relatively low performance. This study demonstrates that deep-learning-based object detection can successfully detect multiple types of solar events, and it may be extended to detect other solar events. In addition, we provide the dataset for further achievements of object detection studies in solar physics.},
	number = {11},
	journal = {Solar Physics},
	author = {Baek, Ji-Hye and Kim, Sujin and Choi, Seonghwan and Park, Jongyeob and Kim, Jihun and Jo, Wonkeun and Kim, Dongil},
	month = nov,
	year = {2021},
	keywords = {SUN, CONVOLUTIONAL neural networks, DEEP learning, OBJECT recognition (Computer vision), Coronal holes, Data management, Event detection, Prominences, SPACE environment, Sunspots},
	pages = {1--15},
}

@article{albert_model-independent_2020,
	title = {Model-independent search for neutrino sources with the {ANTARES} neutrino telescope},
	volume = {114},
	issn = {09276505},
	doi = {10.1016/j.astropartphys.2019.06.003},
	abstract = {A novel method to analyse the spatial distribution of neutrino candidates recorded with the ANTARES neutrino telescope is introduced, searching for an excess of neutrinos in a region of arbitrary size and shape from any direction in the sky. Techniques originating from the domains of machine learning, pattern recognition and image processing are used to purify the sample of neutrino candidates and for the analysis of the obtained skymap. In contrast to a dedicated search for a specific neutrino emission model, this approach is sensitive to a wide range of possible morphologies of potential sources of high-energy neutrino emission. The application of these methods to ANTARES data yields a large-scale excess with a post-trial significance of 2.5 σ. Applied to public data from IceCube in its IC40 configuration, an excess consistent with the results from ANTARES is observed with a post-trial significance of 2.1 σ.},
	journal = {Astroparticle Physics},
	author = {Albert, A. and André, M. and Anghinolfi, M. and Anton, G. and Ardid, M. and Aubert, J.-J. and Avgitas, T. and Baret, B. and Barrios-Martí, J. and Basa, S. and Bertin, V. and Biagi, S. and Bormuth, R. and Bourret, S. and Bouwhuis, M.c. and Bruijn, R. and Brunner, J. and Busto, J. and Capone, A. and Caramete, L.},
	month = jan,
	year = {2020},
	keywords = {IMAGE processing, IMAGE recognition (Computer vision), TELESCOPES, Anisotropy, Astroparticle physics, Neutrino astronomy, NEUTRINO astrophysics, NEUTRINO detectors, NEUTRINOS, PATTERN perception, Pattern recognition, PHOTOMULTIPLIERS},
	pages = {35--47},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\5635UDIB\\Albert et al. - 2020 - Model-independent search for neutrino sources with.pdf:application/pdf},
}

@article{tenbrinck_image_2015,
	title = {Image segmentation with arbitrary noise models by solving minimal surface problems},
	volume = {48},
	issn = {00313203},
	doi = {10.1016/j.patcog.2015.01.006},
	abstract = {Segmentation is one of the fundamental tasks in computer vision applications. For natural images there exist a vast amount of sophisticated segmentation methods in the literature. However, these standard methods tend to fail in the presence of non-Gaussian image noise, e.g., in biomedical imaging or astronomy. In this paper we propose an adequate variational segmentation model for segmentation of images perturbed by arbitrary noise models without adding a-priori assumptions about the unknown physical noise model. For this, we discuss the prominent minimal surface problem and two different numerical minimization schemes to solve it. The first approach efficiently computes the set of all possible minimal surface solutions for the given data via convex optimization and reduces the segmentation problem to the estimation of a proper threshold. The second approach is based on level set methods and is especially suitable for the separation of inhomogeneous image regions. The advantage of this approach is both its simpleness and robustness: the noise in the data does not have to be modeled explicitly since the image intensities are separated using histogram-based thresholding techniques. The proposed model can be interpreted as a generalization of many traditional segmentation methods which implicitly assume a perturbation by additive Gaussian noise. The superiority of this approach over standard methods such as the popular Chan–Vese formulation is demonstrated on synthetic images as well as real application data from biomedical imaging.},
	number = {11},
	journal = {Pattern Recognition},
	author = {Tenbrinck, Daniel and Jiang, Xiaoyi},
	month = nov,
	year = {2015},
	keywords = {COMPUTER vision, IMAGE segmentation, Chan–Vese model, Convex optimization, IMAGE denoising, Level set methods, Minimal surface problem, Physical noise, PLATEAU'S problem, PROBLEM solving ability testing, Segmentation, Thresholding},
	pages = {3293--3309},
}

@article{zimmermann_deep_2019,
	title = {Deep neural networks for classifying complex features in diffraction images},
	volume = {99},
	issn = {24700045},
	doi = {10.1103/PhysRevE.99.063309},
	abstract = {Intense short-wavelength pulses from free-electron lasers and high-harmonic-generation sources enable diffractive imaging of individual nanosized objects with a single x-ray laser shot. The enormous data sets with up to several million diffraction patterns present a severe problem for data analysis because of the high dimensionality of imaging data. Feature recognition and selection is a crucial step to reduce the dimensionality. Usually, custom-made algorithms are developed at a considerable effort to approximate the particular features connected to an individual specimen, but because they face different experimental conditions, these approaches do not generalize well. On the other hand, deep neural networks are the principal instrument for today's revolution in automated image recognition, a development that has not been adapted to its full potential for data analysis in science. We recently published [Langbehn et al., Phys. Rev. Lett. 121, 255301 (2018)] the application of a deep neural network as a feature extractor for wide-angle diffraction images of helium nanodroplets. Here we present the setup, our modifications, and the training process of the deep neural network for diffraction image classification and its systematic bench marking. We find that deep neural networks significantly outperform previous attempts for sorting and classifying complex diffraction patterns and are a significant improvement for the much-needed assistance during postprocessing of large amounts of experimental coherent diffraction imaging data.},
	number = {6},
	journal = {Physical Review E},
	author = {Zimmermann, Julian and Langbehn, Bruno and Cucini, Riccardo and Di Fraia, Michele and Finetti, Paola and LaForge, Aaron C. and {Toshiyuki Nishiyama} and Ovcharenko, Yevheniy,  and Piseri, Paolo and Plekan, Oksana and Prince, Kevin C. and Stienkemeier, Frank and Ueda, Kiyoshi and Callegari, Carlo and Möller, Thomas and Rupp, Daniela},
	month = jun,
	year = {2019},
	keywords = {IMAGE recognition (Computer vision), DATA science, DIFFRACTION patterns, FEATURE selection, HARMONIC generation, X-ray lasers},
	pages = {1--1},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\EUIFG8J5\\Zimmermann et al. - 2019 - Deep neural networks for classifying complex featu.pdf:application/pdf},
}

@article{ribli_weak_2019,
	title = {Weak lensing cosmology with convolutional neural networks on noisy data},
	volume = {490},
	issn = {00358711},
	doi = {10.1093/mnras/stz2610},
	abstract = {Weak gravitational lensing is one of the most promising cosmological probes of the late universe. Several large ongoing (DES, KiDS, HSC) and planned (LSST, Euclid, WFIRST) astronomical surveys attempt to collect even deeper and larger scale data on weak lensing. Due to gravitational collapse, the distribution of dark matter is non-Gaussian on small scales. However, observations are typically evaluated through the two-point correlation function of galaxy shear, which does not capture non-Gaussian features of the lensing maps. Previous studies attempted to extract non-Gaussian information from weak lensing observations through several higher order statistics such as the three-point correlation function, peak counts, or Minkowski functionals. Deep convolutional neural networks (CNN) emerged in the field of computer vision with tremendous success, and they offer a new and very promising framework to extract information from 2D or 3D astronomical data sets, confirmed by recent studies on weak lensing. We show that a CNN is able to yield significantly stricter constraints of (σ8, Ωm) cosmological parameters than the power spectrum using convergence maps generated by full N -body simulations and ray-tracing, at angular scales and shape noise levels relevant for future observations. In a scenario mimicking LSST or Euclid , the CNN yields 2.4–2.8 times smaller credible contours than the power spectrum, and 3.5–4.2 times smaller at noise levels corresponding to a deep space survey such as WFIRST. We also show that at shape noise levels achievable in future space surveys the CNN yields 1.4–2.1 times smaller contours than peak counts, a higher order statistic capable of extracting non-Gaussian information from weak lensing maps.},
	number = {2},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Ribli, Dezső and Pataki, Bálint Ármin and Zorrilla Matilla, José Manuel and Hsu, Daniel and Haiman, Zoltán and Csabai, István},
	month = dec,
	year = {2019},
	keywords = {ARTIFICIAL neural networks, ASTRONOMICAL surveys, techniques: image processing, COSMIC background radiation, dark matter, DARK matter, EUCLID, FRIEDMANN equations, GRAVITATIONAL lenses, gravitational lensing: weak},
	pages = {1843--1860},
	file = {Submitted Version:C\:\\Users\\annab\\Zotero\\storage\\7IW8HLU6\\Ribli et al. - 2019 - Weak lensing cosmology with convolutional neural n.pdf:application/pdf},
}

@article{sabin_first_2021,
	title = {First deep images catalogue of extended {IPHAS} {PNe}},
	volume = {508},
	issn = {00358711},
	doi = {10.1093/mnras/stab2477},
	abstract = {We present the first instalment of a deep imaging catalogue containing 58 True, Likely, and Possible extended PNe detected with the Isaac Newton Telescope Photometric H α Survey (IPHAS). The three narrow-band filters in the emission lines of H α, [N  ii ] λ6584 Å, and [O  iii ] λ5007 Å used for this purpose allowed us to improve our description of the morphology and dimensions of the nebulae. In some cases even the nature of the source has been reassessed. We were then able to unveil new macro- and micro-structures, which will without a doubt contribute to a more accurate analysis of these PNe. It has been also possible to perform a primary classification of the targets based on their ionization level. A Deep Learning classification tool has also been tested. We expect that all the PNe from the IPHAS catalogue of new extended planetary nebulae will ultimately be part of this deep H α, [N  ii ], and [O  iii ] imaging catalogue.},
	number = {2},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Sabin, L and Guerrero, M A and Ramos-Larios, G and Boumis, P and Zijlstra, A A and Awang Iskandar, D N F and Barlow, M J and Toalá, J A and Parker, Q A and Corradi, R M L and Morris, R a H},
	month = dec,
	year = {2021},
	keywords = {NEBULAE, STELLAR winds, PLANETARY nebulae, DEEP learning, CATALOGING, CATALOGS, NEWTON, Isaac, 1642-1727, outflows, planetary nebulae, stars: mass-loss, stars: winds},
	pages = {1599--1617},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\AV4ALM9S\\Sabin et al. - 2021 - First deep images catalogue of extended IPHAS PNe.pdf:application/pdf},
}

@article{szabo_machine_2022,
	title = {Machine learning in present day astrophysics},
	volume = {53},
	issn = {05317479},
	doi = {10.1051/epn/2022205},
	abstract = {Machine learning is everywhere in our daily life. From the social media and bank sector to transportation and telecommunication, we cannot avoid using it, sometimes even without noticing that we are relying on it. Astronomy and astrophysics are no exception. From telescope time and survey telescope scheduling through object detection and classification, to cleaning images and making large simulations smarter and quicker to it is ubiquitous to use machine learning algorithms. To illustrate this silent revolution, we checked the NASA Astronomical Data System website and searched for the keyword 'machine learning' in abstracts of astronomical and astrophysical papers. In 2000 we found 56, in 2010 889, and by 2020 no less than 35,659 abstracts contained the magic two words.},
	number = {2},
	journal = {Europhysics News},
	author = {Szabó, R. and Szklenár, T. and Bódi, A.},
	month = may,
	year = {2022},
	keywords = {ASTRONOMY, ASTROPHYSICS, MACHINE learning, OBJECT recognition (Computer vision), KEYWORD searching, TELECOMMUNICATION, UNITED States. National Aeronautics \& Space Administration},
	pages = {22--25},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\7S5AEHLS\\Szabó et al. - 2022 - Machine learning in present day astrophysics.pdf:application/pdf},
}

@article{zhan-wei_space_2016,
	title = {Space {Object} {Tracking} {Method} {Based} on a {Snake} {Model}},
	volume = {40},
	issn = {02751062},
	doi = {10.1016/j.chinastron.2016.05.004},
	abstract = {In this paper, aiming at the problem of unstable tracking of low-orbit variable and bright space objects, adopting an active contour model, a kind of improved GVF (Gradient Vector Flow) - Snake algorithm is proposed to realize the real-time search of the real object contour on the CCD image. Combined with the Kalman filter for prediction, a new adaptive tracking method is proposed for space objects. Experiments show that this method can overcome the tracking error caused by the fixed window, and improve the tracking robustness.},
	number = {2},
	journal = {Chinese Astronomy \& Astrophysics},
	author = {Zhan-wei, Xu and Xin, Wang},
	month = apr,
	year = {2016},
	keywords = {COMPUTER vision, techniques: image processing, ARTIFICIAL satellite tracking, KALMAN filtering, MATHEMATICAL control theory, OBJECT tracking (Computer vision), space vehicles, telescopes},
	pages = {266--276},
}

@article{mohammadi_manifold_2022,
	title = {Manifold {Alignment} {Aware} {Ants}: {A} {Markovian} {Process} for {Manifold} {Extraction}},
	volume = {34},
	issn = {08997667},
	shorttitle = {Manifold {Alignment} {Aware} {Ants}},
	doi = {10.1162/neco_a_01478},
	abstract = {The presence of manifolds is a common assumption in many applications, including astronomy and computer vision. For instance, in astronomy, low-dimensional stellar structures, such as streams, shells, and globular clusters, can be found in the neighborhood of big galaxies such as the Milky Way. Since these structures are often buried in very large data sets, an algorithm, which can not only recover the manifold but also remove the background noise (or outliers), is highly desirable. While other works try to recover manifolds either by pushing all points toward manifolds or by downsampling from dense regions, aiming to solve one of the problems, they generally fail to suppress the noise on manifolds and remove background noise simultaneously. Inspired by the collective behavior of biological ants in food-seeking process, we propose a new algorithm that employs several random walkers equipped with a local alignment measure to detect and denoise manifolds. During the walking process, the agents release pheromone on data points, which reinforces future movements. Over time the pheromone concentrates on the manifolds, while it fades in the background noise due to an evaporation procedure. We use the Markov chain (MC) framework to provide a theoretical analysis of the convergence of the algorithm and its performance. Moreover, an empirical analysis, based on synthetic and real-world data sets, is provided to demonstrate its applicability in different areas, such as improving the performance of t -distributed stochastic neighbor embedding (t-SNE) and spectral clustering using the underlying MC formulas, recovering astronomical low-dimensional structures, and improving the performance of the fast Parzen window density estimator.},
	number = {3},
	journal = {Neural Computation},
	author = {Mohammadi, Mohammad and Tino, Peter and Bunte, Kerstin},
	month = mar,
	year = {2022},
	keywords = {MILKY Way, BIG data, ALGORITHM performance analysis, COLLECTIVE behavior, GLOBULAR clusters, STELLAR structure},
	pages = {595--641},
}

@article{hao_gradient-aligned_2022,
	title = {Gradient-{Aligned} convolution neural network},
	volume = {122},
	issn = {00313203},
	doi = {10.1016/j.patcog.2021.108354},
	abstract = {• We propose a general Convolution operation, called GAConv, which can replace conventional operations in CNN to help it achieve rotation invariance. • With GAConv, Gradient-Aligned CNN (GACNN) can achieve rotation invariance without any data augmentation, feature-map augmentation, and filter enrichment. • In GACNN, rotation invariance does not learn from the training set, but bases on the network model. Different from the vanilla CNN, GACNN will output invariant results for all rotated versions of an object, no matter whether the network is trained or not. • We conduct classification experiments on designed dataset and realistic datasets. The results show that with the same computation cost, GACNN achieved better results than conventional CNN and some rotational invariant CNN. Although Convolution Neural Networks (CNN) have achieved great success in many applications of computer vision in recent years, rotation invariance is still a difficult problem for CNN. Especially for some images, the content can appear in the image at any angle of rotation, such as medical images, microscopic images, remote sensing images and astronomical images. In this paper, we propose a novel convolution operation, called Gradient-Aligned Convolution (GAConv), which can help CNN achieve rotation invariance by replacing vanilla convolutions in CNN. GAConv is implemented with a prior pixel-level gradient alignment operation before regular convolution. With GAConv, Gradient-Aligned CNN (GACNN) can achieve rotation invariance without any data augmentation, feature-map augmentation, and filter enrichment. In GACNN, rotation invariance does not learn from the training set, but bases on the network model. Different from the vanilla CNN, GACNN will output invariant results for all rotated versions of an object, no matter whether the network is trained or not. This means that we only need to train the network with one canonical version of the object and all other rotated versions of this object should be recognized with the same accuracy. Classification experiments have been conducted to evaluate GACNN compared with some rotation invariant approaches. GACNN achieved the best results on the 360 ∘ rotated test set of MNIST-rotation, Plankton-sub-rotation, and Galaxy Zoo 2.},
	journal = {Pattern Recognition},
	author = {Hao, You and Hu, Ping and Li, Shirui and Udupa, Jayaram K. and Tong, Yubing and Li, Hua},
	month = feb,
	year = {2022},
	keywords = {COMPUTER vision, CONVOLUTIONAL neural networks, REMOTE sensing, DATA augmentation, Gradient alignment, Rotation equivariant convolution, Rotation invariant neural network},
	pages = {N.PAG--N.PAG},
}

@article{bom_deep_2021,
	title = {Deep {Learning} assessment of galaxy morphology in {S}-{PLUS} {Data} {Release} 1},
	volume = {507},
	issn = {00358711},
	doi = {10.1093/mnras/stab1981},
	abstract = {The morphological diversity of galaxies is a relevant probe of galaxy evolution and cosmological structure formation, but the classification of galaxies in large sky surveys is becoming a significant challenge. We use data from the Stripe-82 area observed by the Southern Photometric Local Universe Survey (S-PLUS) in 12 optical bands, and present a catalogue of the morphologies of galaxies brighter than r = 17 mag determined both using a novel multiband morphometric fitting technique and Convolutional Neural Networks (CNNs) for computer vision. Using the CNNs, we find that, compared to our baseline results with three bands, the performance increases when using 5 broad and 3 narrow bands, but is poorer when using the full 12 band S-PLUS image set. However, the best result is still achieved with just three optical bands when using pre-trained network weights from an ImageNet data set. These results demonstrate the importance of using prior knowledge about neural network weights based on training in unrelated, extensive data sets, when available. Our catalogue contains 3274 galaxies in Stripe-82 that are not present in Galaxy Zoo 1 (GZ1), and we also provide our classifications for 4686 galaxies that were considered ambiguous in GZ1. Finally, we present a prospect of a novel way to take advantage of 12 band information for morphological classification using morphometric features, and we release a model that has been pre-trained on several bands that could be adapted for classifications using data from other surveys. The morphological catalogues are publicly available.},
	number = {2},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Bom, C R and Cortesi, A and Lucatelli, G and Dias, L O and Schubert, P and Oliveira Schwarz, G B and Cardoso, N M and Lima, E V R and Mendes de Oliveira, C and Sodre, L and Smith Castelli, A V and Ferrari, F and Damke, G and Overzier, R and Kanaan, A and Ribeiro, T and Schoenell, W},
	month = oct,
	year = {2021},
	keywords = {CONVOLUTIONAL neural networks, DEEP learning, surveys, techniques: image processing, CLASSIFICATION of galaxies, GALACTIC evolution, galaxies: fundamental parameters, galaxies: structure, methods: miscellaneous, MORPHOLOGY},
	pages = {1937--1955},
	file = {Accepted Version:C\:\\Users\\annab\\Zotero\\storage\\X5DT6DFF\\Bom et al. - 2021 - Deep Learning assessment of galaxy morphology in S.pdf:application/pdf},
}

@article{banerjee_galaxy_2021,
	title = {Galaxy {Morphological} {Image} {Classification} using {ResNet}},
	volume = {62},
	issn = {00672904},
	doi = {10.24996/ijs.2021.62.10.27},
	abstract = {Machine learning-based techniques are used widely for the classification of images into various categories. The advancement of Convolutional Neural Network (CNN) affects the field of computer vision on a large scale. It has been applied to classify and localize objects in images. Among the fields of applications of CNN, it has been applied to understand huge unstructured astronomical data being collected every second. Galaxies have diverse and complex shapes and their morphology carries fundamental information about the whole universe. Studying these galaxies has been a tremendous task for the researchers around the world. Researchers have already applied some basic CNN models to predict the morphological classes of the galaxies. In this paper, a residual network (ResNet) model is applied for this purpose. The proposed methodology classified the galaxies depending on their shape into 37 different classes. The performance of the methodology was evaluated using the data set provided by Kaggle. In this data set, 61,578 galaxy images are given, which are classified by human eye. The model achieved nearly 98\% accuracy.},
	number = {10},
	journal = {Iraqi Journal of Science},
	author = {Banerjee, Siddhartha and Ghosh, Bibek Ranjan and Gangapadhyay, Ayan and Chatterjee, Himadri Sankar},
	month = oct,
	year = {2021},
	keywords = {GALAXIES, CONVOLUTIONAL neural networks, MACHINE learning, ASTRONOMICAL models, Convolutional Neural Network, Galaxy Morphological Classification, Galaxy Zoo Data set, Residual Networks, ResNet-18, UNIVERSE},
	pages = {3690--3696},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\LJFLZY4V\\Banerjee et al. - 2021 - Galaxy Morphological Image Classification using Re.pdf:application/pdf},
}

@article{shamir_automatic_2009,
	title = {Automatic morphological classification of galaxy images},
	volume = {399},
	issn = {00358711},
	doi = {10.1111/j.1365-2966.2009.15366.x},
	abstract = {We describe an image analysis supervised learning algorithm that can automatically classify galaxy images. The algorithm is first trained using manually classified images of elliptical, spiral and edge-on galaxies. A large set of image features is extracted from each image, and the most informative features are selected using Fisher scores. Test images can then be classified using a simple Weighted Nearest Neighbour rule such that the Fisher scores are used as the feature weights. Experimental results show that galaxy images from Galaxy Zoo can be classified automatically to spiral, elliptical and edge-on galaxies with an accuracy of ∼90 per cent compared to classifications carried out by the author. Full compilable source code of the algorithm is available for free download, and its general-purpose nature makes it suitable for other uses that involve automatic image analysis of celestial objects.},
	number = {3},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Shamir, Lior},
	month = nov,
	year = {2009},
	keywords = {ASTRONOMY, GALAXIES, MACHINE learning, ALGORITHMS, methods: data analysis, techniques: image processing, IMAGE analysis},
	pages = {1367--1372},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\CCQMMVEM\\Shamir - 2009 - Automatic morphological classification of galaxy i.pdf:application/pdf},
}

@article{m_ntampaka_deep_2019,
	title = {A {Deep} {Learning} {Approach} to {Galaxy} {Cluster} {X}-{Ray} {Masses}},
	volume = {876},
	issn = {0004637X},
	doi = {10.3847/1538-4357/ab14eb},
	abstract = {We present a machine-learning (ML) approach for estimating galaxy cluster masses from Chandra mock images. We utilize a Convolutional Neural Network (CNN), a deep ML tool commonly used in image recognition tasks. The CNN is trained and tested on our sample of 7896 Chandra X-ray mock observations, which are based on 329 massive clusters from the simulation. Our CNN learns from a low resolution spatial distribution of photon counts and does not use spectral information. Despite our simplifying assumption to neglect spectral information, the resulting mass values estimated by the CNN exhibit small bias in comparison to the true masses of the simulated clusters (−0.02 dex) and reproduce the cluster masses with low intrinsic scatter, 8\% in our best fold and 12\% averaging over all. In contrast, a more standard core-excised luminosity method achieves 15\%–18\% scatter. We interpret the results with an approach inspired by Google DeepDream and find that the CNN ignores the central regions of clusters, which are known to have high scatter with mass.},
	number = {1},
	journal = {Astrophysical Journal},
	author = {{M. Ntampaka} and {J. ZuHone} and {D. Eisenstein} and {D. Nagai} and {A. Vikhlinin} and {L. Hernquist} and {F. Marinacci} and {D. Nelson} and {R. Pakmor} and {A. Pillepich} and {P. Torrey} and {M. Vogelsberger}},
	year = {2019},
	keywords = {ARTIFICIAL neural networks, DEEP learning, IMAGE recognition (Computer vision), GALAXY clusters, PHOTON counting, X-rays},
	pages = {1--1},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\REUJKVYC\\M. Ntampaka et al. - 2019 - A Deep Learning Approach to Galaxy Cluster X-Ray M.pdf:application/pdf},
}

@techreport{jia_detection_2022,
	title = {Detection of {Strongly} {Lensed} {Arcs} in {Galaxy} {Clusters} with {Transformers}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv221105972J},
	abstract = {Strong lensing in galaxy clusters probes properties of dense cores of dark matter halos in mass, studies the distant universe at flux levels and spatial resolutions otherwise unavailable, and constrains cosmological models independently. The next-generation large scale sky imaging surveys are expected to discover thousands of cluster-scale strong lenses, which would lead to unprecedented opportunities for applying cluster-scale strong lenses to solve astrophysical and cosmological problems. However, the large dataset challenges astronomers to identify and extract strong lensing signals, particularly strongly lensed arcs, because of their complexity and variety. Hence, we propose a framework to detect cluster-scale strongly lensed arcs, which contains a transformer-based detection algorithm and an image simulation algorithm. We embed prior information of strongly lensed arcs at cluster-scale into the training data through simulation and then train the detection algorithm with simulated images. We use the trained transformer to detect strongly lensed arcs from simulated and real data. Results show that our approach could achieve 99.63 \% accuracy rate, 90.32 \% recall rate, 85.37 \% precision rate and 0.23 \% false positive rate in detection of strongly lensed arcs from simulated images and could detect almost all strongly lensed arcs in real observation images. Besides, with an interpretation method, we have shown that our method could identify important information embedded in simulated data. Next step, to test the reliability and usability of our approach, we will apply it to available observations (e.g., DESI Legacy Imaging Surveys) and simulated data of upcoming large-scale sky surveys, such as the Euclid and the CSST.},
	urldate = {2022-11-14},
	author = {Jia, Peng and Sun, Ruiqi and Li, Nan and Song, Yu and Ning, Runyu and Wei, Hongyan and Luo, Rui},
	month = nov,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv221105972J
Type: article},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\BT3KH8TM\\Jia et al. - 2022 - Detection of Strongly Lensed Arcs in Galaxy Cluste.pdf:application/pdf},
}

@techreport{li_galaxy_2022,
	title = {Galaxy {Image} {Deconvolution} for {Weak} {Gravitational} {Lensing} with {Physics}-informed {Deep} {Learning}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv221101567L},
	abstract = {Removing optical and atmospheric blur from galaxy images significantly improves galaxy shape measurements for weak gravitational lensing and galaxy evolution studies. This ill-posed linear inverse problem is usually solved with deconvolution algorithms enhanced by regularisation priors or deep learning. We introduce a so-called "physics-based deep learning" approach to the Point Spread Function (PSF) deconvolution problem in galaxy surveys. We apply algorithm unrolling and the Plug-and-Play technique to the Alternating Direction Method of Multipliers (ADMM) with a Poisson noise model and use a neural network to learn appropriate priors from simulated galaxy images. We characterise the time-performance trade-off of several methods for galaxies of differing brightness levels, showing an improvement of 26\% (SNR=20)/48\% (SNR=100) compared to standard methods and 14\% (SNR=20) compared to modern methods.},
	urldate = {2022-11-14},
	author = {Li, Tianao and Alexander, Emma},
	month = nov,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv221101567L
Type: article},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\HZ4TSYTB\\Li and Alexander - 2022 - Galaxy Image Deconvolution for Weak Gravitational .pdf:application/pdf},
}

@techreport{ciprijanovic_semi-supervised_2022,
	title = {Semi-{Supervised} {Domain} {Adaptation} for {Cross}-{Survey} {Galaxy} {Morphology} {Classification} and {Anomaly} {Detection}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv221100677C},
	abstract = {In the era of big astronomical surveys, our ability to leverage artificial intelligence algorithms simultaneously for multiple datasets will open new avenues for scientific discovery. Unfortunately, simply training a deep neural network on images from one data domain often leads to very poor performance on any other dataset. Here we develop a Universal Domain Adaptation method DeepAstroUDA, capable of performing semi-supervised domain alignment that can be applied to datasets with different types of class overlap. Extra classes can be present in any of the two datasets, and the method can even be used in the presence of unknown classes. For the first time, we demonstrate the successful use of domain adaptation on two very different observational datasets (from SDSS and DECaLS). We show that our method is capable of bridging the gap between two astronomical surveys, and also performs well for anomaly detection and clustering of unknown data in the unlabeled dataset. We apply our model to two examples of galaxy morphology classification tasks with anomaly detection: 1) classifying spiral and elliptical galaxies with detection of merging galaxies (three classes including one unknown anomaly class); 2) a more granular problem where the classes describe more detailed morphological properties of galaxies, with the detection of gravitational lenses (ten classes including one unknown anomaly class).},
	urldate = {2022-11-14},
	author = {Ćiprijanović, Aleksandra and Lewis, Ashia and Pedro, Kevin and Madireddy, Sandeep and Nord, Brian and Perdue, Gabriel N. and Wild, Stefan},
	month = nov,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv221100677C
Type: article},
	keywords = {Astrophysics - Astrophysics of Galaxies, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\KPXAKD9D\\Ćiprijanović et al. - 2022 - Semi-Supervised Domain Adaptation for Cross-Survey.pdf:application/pdf},
}

@techreport{huang_strong_2022,
	title = {Strong {Gravitational} {Lensing} {Parameter} {Estimation} with {Vision} {Transformer}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv221004143H},
	abstract = {Quantifying the parameters and corresponding uncertainties of hundreds of strongly lensed quasar systems holds the key to resolving one of the most important scientific questions: the Hubble constant (\$H\_\{0\}\$) tension. The commonly used Markov chain Monte Carlo (MCMC) method has been too time-consuming to achieve this goal, yet recent work has shown that convolution neural networks (CNNs) can be an alternative with seven orders of magnitude improvement in speed. With 31,200 simulated strongly lensed quasar images, we explore the usage of Vision Transformer (ViT) for simulated strong gravitational lensing for the first time. We show that ViT could reach competitive results compared with CNNs, and is specifically good at some lensing parameters, including the most important mass-related parameters such as the center of lens \${\textbackslash}theta\_\{1\}\$ and \${\textbackslash}theta\_\{2\}\$, the ellipticities \$e\_1\$ and \$e\_2\$, and the radial power-law slope \${\textbackslash}gamma'\$. With this promising preliminary result, we believe the ViT (or attention-based) network architecture can be an important tool for strong lensing science for the next generation of surveys. The open source of our code and data is in {\textbackslash}url\{https://github.com/kuanweih/strong\_lensing\_vit\_resnet\}.},
	urldate = {2022-11-14},
	author = {Huang, Kuan-Wei and Chih-Fan Chen, Geoff and Chang, Po-Wen and Lin, Sheng-Chieh and Hsu, Chia-Jung and Thengane, Vishal and Yao-Yu Lin, Joshua},
	month = oct,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv221004143H
Type: article},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\TYZFUKJT\\Huang et al. - 2022 - Strong Gravitational Lensing Parameter Estimation .pdf:application/pdf},
}

@article{petrillo_vizier_2022,
	title = {{VizieR} {Online} {Data} {Catalog}: {Galaxy}-scale lenses in the {Kilo}-{Degree} {Survey} ({Petrillo}+, 2019)},
	shorttitle = {{VizieR} {Online} {Data} {Catalog}},
	url = {https://ui.adsabs.harvard.edu/abs/2022yCat..74843879P},
	abstract = {The Kilo-Degree Survey (KiDS; de Jong et al. 2013ExA....35...25D) is an ESO public survey carried out with the OmegaCAM wide-field imager (Kuijken 2011Msngr.146....8K) mounted on the VLT Survey Telescope (VST; Capaccioli \& Schipani 2011Msngr.146....2C) at the Paranal Observatory in Chile. The telescope, camera, and survey have been designed to obtain images with subarcsecond seeing and homogeneous image quality both across the full field of view and throughout the survey execution. To find gravitational lens candidates in KiDS imaging data, we use the ConvNets previously introduced by Petrillo et al. (2019MNRAS.482..807P). These networks are significantly improved variants of the original ConvNet presented by Petrillo et al. (2017MNRAS.472.1129P). ConvNets represent a state-of-the-art method of pattern recognition. The networks learn how to classify a diverse set of images during the so-called training phase, whereby labelled images are provided to the ConvNet. Its weight parameters are changed to minimize a pre-defined loss function, which expresses the difference between the labels of the images and the output values p (one for each image) of the ConvNet. We present in table A1 the 'Lenses in the Kilo-Degree Survey (LinKS) sample' as the full sample of 1983 gravitational lens candidates retrieved with p{\textgreater}0.8 and a score from the visual inspection greater than zero. (1 data file).},
	urldate = {2022-11-14},
	journal = {VizieR Online Data Catalog},
	author = {Petrillo, C. E. and Tortora, C. and Vernardos, G. and Koopmans, L. V. E. and Verdoes Kleijn, G. and Bilicki, M. and Napolitano, N. R. and Chatterjee, S. and Covone, G. and Dvornik, A. and Erben, T. and Getman, F. and Giblin, B. and Heymans, C. and de Jong, J. T. A. and Kuijken, K. and Schneider, P. and Shan, H. and Spiniello, C. and Wright, A. H.},
	month = sep,
	year = {2022},
	note = {ADS Bibcode: 2022yCat..74843879P},
	keywords = {Galaxies, Gravitational lensing, Positional data},
	pages = {J/MNRAS/484/3879},
}

@techreport{troxel_joint_2022,
	title = {A {Joint} {Roman} {Space} {Telescope} and {Rubin} {Observatory} {Synthetic} {Wide}-{Field} {Imaging} {Survey}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv220906829T},
	abstract = {We present and validate 20 deg\${\textasciicircum}2\$ of overlapping synthetic imaging surveys representing the full depth of the Nancy Grace Roman Space Telescope High-Latitude Imaging Survey (HLIS) and five years of observations of the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST). The two synthetic surveys are summarized, with reference to the existing 300 deg\${\textasciicircum}2\$ of LSST simulated imaging produced as part of Dark Energy Science Collaboration (DESC) Data Challenge 2 (DC2). Both synthetic surveys observe the same simulated DESC DC2 universe. For the synthetic Roman survey, we simulate for the first time fully chromatic images along with the detailed physics of the Sensor Chip Assemblies derived from lab measurements using the flight detectors. The simulated imaging and resulting pixel-level measurements of photometric properties of objects span a wavelength range of \${\textbackslash}sim\$0.3 to 2.0 \${\textbackslash}mu\$m. We also describe updates to the Roman simulation pipeline, changes in how astrophysical objects are simulated relative to the original DC2 simulations, and the resulting simulated Roman data products. We use these simulations to explore the relative fraction of unrecognized blends in LSST images, finding that 20-30\% of objects identified in LSST images with \$i\$-band magnitudes brighter than 25 can be identified as multiple objects in Roman images. These simulations provide a unique testing ground for the development and validation of joint pixel-level analysis techniques of ground- and space-based imaging data sets in the second half of the 2020s -- in particular the case of joint Roman--LSST analyses.},
	urldate = {2022-11-14},
	author = {Troxel, M. A. and Lin, C. and Park, A. and Hirata, C. and Mandelbaum, R. and Jarvis, M. and Choi, A. and Givans, J. and Higgins, M. and Sanchez, B. and Yamamoto, M. and Awan, H. and Chiang, J. and Dore, O. and Walter, C. W. and Zhang, T. and Cohen-Tanugi, J. and Gawiser, E. and Hearin, A. and Heitmann, K. and Ishak, M. and Kovacs, E. and Mao, Y. -Y. and Wood-Vasey, M. and {the LSST Dark Energy Science Collaboration}},
	month = sep,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv220906829T
Type: article},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\I49KKAKQ\\Troxel et al. - 2022 - A Joint Roman Space Telescope and Rubin Observator.pdf:application/pdf},
}

@article{han_identifying_2022,
	title = {Identifying {Outliers} in {Astronomical} {Images} with {Unsupervised} {Machine} {Learning}},
	volume = {22},
	issn = {1674-4527},
	url = {https://ui.adsabs.harvard.edu/abs/2022RAA....22h5006H},
	doi = {10.1088/1674-4527/ac7386},
	abstract = {Astronomical outliers, such as unusual, rare or unknown types of astronomical objects or phenomena, constantly lead to the discovery of genuinely unforeseen knowledge in astronomy. More unpredictable outliers will be uncovered in principle with the increment of the coverage and quality of upcoming survey data. However, it is a severe challenge to mine rare and unexpected targets from enormous data with human inspection due to a significant workload. Supervised learning is also unsuitable for this purpose because designing proper training sets for unanticipated signals is unworkable. Motivated by these challenges, we adopt unsupervised machine learning approaches to identify outliers in the data of galaxy images to explore the paths for detecting astronomical outliers. For comparison, we construct three methods, which are built upon the k-nearest neighbors (KNN), Convolutional Auto-Encoder (CAE) + KNN, and CAE + KNN + Attention Mechanism (attCAE\_KNN) separately. Testing sets are created based on the Galaxy Zoo image data published online to evaluate the performance of the above methods. Results show that attCAE\_KNN achieves the best recall (78\%), which is 53\% higher than the classical KNN method and 22\% higher than CAE+KNN. The efficiency of attCAE\_KNN (10 minutes) is also superior to KNN (4 h) and equal to CAE+KNN (10 minutes) for accomplishing the same task. Thus, we believe that it is feasible to detect astronomical outliers in the data of galaxy images in an unsupervised manner. Next, we will apply attCAE\_KNN to available survey data sets to assess its applicability and reliability.},
	urldate = {2022-11-14},
	journal = {Research in Astronomy and Astrophysics},
	author = {Han, Yang and Zou, Zhiqiang and Li, Nan and Chen, Yanli},
	month = aug,
	year = {2022},
	note = {ADS Bibcode: 2022RAA....22h5006H},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Computer Vision and Pattern Recognition, Galaxy, Galaxy: fundamental parameters, Physical Data and Processes},
	pages = {085006},
	file = {Submitted Version:C\:\\Users\\annab\\Zotero\\storage\\8WAU85PW\\Han et al. - 2022 - Identifying Outliers in Astronomical Images with U.pdf:application/pdf},
}

@article{nourbakhsh_galaxy_2022,
	title = {Galaxy blending effects in deep imaging cosmic shear probes of cosmology},
	volume = {514},
	issn = {0035-8711},
	url = {https://ui.adsabs.harvard.edu/abs/2022MNRAS.514.5905N},
	doi = {10.1093/mnras/stac1303},
	abstract = {Upcoming deep imaging surveys such as the Vera C. Rubin Observatory Legacy Survey of Space and Time will be confronted with challenges that come with increased depth. One of the leading systematic errors in deep surveys is the blending of objects due to higher surface density in the more crowded images; a considerable fraction of the galaxies which we hope to use for cosmology analyses will overlap each other on the observed sky. In order to investigate these challenges, we emulate blending in a mock catalogue consisting of galaxies at a depth equivalent to 1.3 yr of the full 10-yr Rubin Observatory that includes effects due to weak lensing, ground-based seeing, and the uncertainties due to extraction of catalogues from imaging data. The emulated catalogue indicates that approximately 12 per cent of the observed galaxies are 'unrecognized' blends that contain two or more objects but are detected as one. Using the positions and shears of half a billion distant galaxies, we compute shear-shear correlation functions after selecting tomographic samples in terms of both spectroscopic and photometric redshift bins. We examine the sensitivity of the cosmological parameter estimation to unrecognized blending employing both jackknife and analytical Gaussian covariance estimators. An {\textasciitilde}0.025 decrease in the derived structure growth parameter S8 = σ8(Ωm/0.3)0.5 is seen due to unrecognized blending in both tomographies with a slight additional bias for the photo-z-based tomography. This bias is greater than the 2σ statistical error in measuring S8.},
	urldate = {2022-11-14},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Nourbakhsh, Erfan and Tyson, J. Anthony and Schmidt, Samuel J. and Armstrong, Bob and Burchat, Patricia and Sánchez, Javier and {LSST Dark Energy Science Collaboration}},
	month = aug,
	year = {2022},
	note = {ADS Bibcode: 2022MNRAS.514.5905N},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, cosmological parameters, cosmology: observations, dark matter, gravitational lensing: weak, large-scale structure of Universe, techniques: image processing},
	pages = {5905--5926},
	file = {Submitted Version:C\:\\Users\\annab\\Zotero\\storage\\WZ24LNPC\\Nourbakhsh et al. - 2022 - Galaxy blending effects in deep imaging cosmic she.pdf:application/pdf},
}

@article{soroka_morphological_2022,
	title = {Morphological {Classification} of {Astronomical} {Images} with {Limited} {Labelling}},
	volume = {532},
	url = {https://ui.adsabs.harvard.edu/abs/2022ASPC..532..307S},
	abstract = {The task of morphological classification is complex for simple parameterization, but important for research in the galaxy evolution field. Future galaxy surveys (e.g. EUCLID) will collect data about more than a 109 galaxies. To obtain morphological information one needs to involve people to mark up galaxy images, which requires either a considerable amount of money or a huge number of volunteers. We propose an effective semi-supervised approach for galaxy morphology classification task, based on active learning of adversarial autoencoder (AAE) model. For a binary classification problem (top level question of Galaxy Zoo 2 decision tree) we achieved accuracy 93.1\% on the test part with only 0.86 millions markup actions, this model can easily scale up on any number of images. Our best model with additional markup achieves accuracy of 95.5\%. To the best of our knowledge it is a first time AAE semi-supervised learning model used in astronomy.},
	urldate = {2022-11-14},
	author = {Soroka, Andrey and Meshcheryakov, Alex and Gerasimov, Sergey},
	month = jul,
	year = {2022},
	note = {Conference Name: Astronomical Society of the Pacific Conference Series
Place: eprint: arXiv:2105.02958
ADS Bibcode: 2022ASPC..532..307S},
	keywords = {Astrophysics - Astrophysics of Galaxies, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {307},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\U9VXWZLW\\Soroka et al. - 2022 - Morphological Classification of Astronomical Image.pdf:application/pdf},
}

@article{walmsley_practical_2022,
	title = {Practical galaxy morphology tools from deep supervised representation learning},
	volume = {513},
	issn = {0035-8711},
	url = {https://ui.adsabs.harvard.edu/abs/2022MNRAS.513.1581W},
	doi = {10.1093/mnras/stac525},
	abstract = {Astronomers have typically set out to solve supervised machine learning problems by creating their own representations from scratch. We show that deep learning models trained to answer every Galaxy Zoo DECaLS question learn meaningful semantic representations of galaxies that are useful for new tasks on which the models were never trained. We exploit these representations to outperform several recent approaches at practical tasks crucial for investigating large galaxy samples. The first task is identifying galaxies of similar morphology to a query galaxy. Given a single galaxy assigned a free text tag by humans (e.g. '\#diffuse'), we can find galaxies matching that tag for most tags. The second task is identifying the most interesting anomalies to a particular researcher. Our approach is 100 per cent accurate at identifying the most interesting 100 anomalies (as judged by Galaxy Zoo 2 volunteers). The third task is adapting a model to solve a new task using only a small number of newly labelled galaxies. Models fine-tuned from our representation are better able to identify ring galaxies than models fine-tuned from terrestrial images (ImageNet) or trained from scratch. We solve each task with very few new labels; either one (for the similarity search) or several hundred (for anomaly detection or fine-tuning). This challenges the longstanding view that deep supervised methods require new large labelled data sets for practical use in astronomy. To help the community benefit from our pretrained models, we release our fine-tuning code zoobot. Zoobot is accessible to researchers with no prior experience in deep learning.},
	urldate = {2022-11-14},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Walmsley, Mike and Scaife, Anna M. M. and Lintott, Chris and Lochner, Michelle and Etsebeth, Verlon and Géron, Tobias and Dickinson, Hugh and Fortson, Lucy and Kruk, Sandor and Masters, Karen L. and Mantha, Kameswara Bharadwaj and Simmons, Brooke D.},
	month = jun,
	year = {2022},
	note = {ADS Bibcode: 2022MNRAS.513.1581W},
	keywords = {Astrophysics - Astrophysics of Galaxies, Computer Science - Computer Vision and Pattern Recognition, galaxies: evolution, galaxies: general, methods: data analysis, software: data analysis, software: public release},
	pages = {1581--1599},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\6JPT9QTV\\Walmsley et al. - 2022 - Practical galaxy morphology tools from deep superv.pdf:application/pdf},
}

@techreport{ciprijanovic_robustness_2021,
	title = {Robustness of deep learning algorithms in astronomy -- galaxy morphology studies},
	url = {https://ui.adsabs.harvard.edu/abs/2021arXiv211100961C},
	abstract = {Deep learning models are being increasingly adopted in wide array of scientific domains, especially to handle high-dimensionality and volume of the scientific data. However, these models tend to be brittle due to their complexity and overparametrization, especially to the inadvertent adversarial perturbations that can appear due to common image processing such as compression or blurring that are often seen with real scientific data. It is crucial to understand this brittleness and develop models robust to these adversarial perturbations. To this end, we study the effect of observational noise from the exposure time, as well as the worst case scenario of a one-pixel attack as a proxy for compression or telescope errors on performance of ResNet18 trained to distinguish between galaxies of different morphologies in LSST mock data. We also explore how domain adaptation techniques can help improve model robustness in case of this type of naturally occurring attacks and help scientists build more trustworthy and stable models.},
	urldate = {2022-11-14},
	author = {Ćiprijanović, A. and Kafkes, D. and Perdue, G. N. and Pedro, K. and Snyder, G. and Sánchez, F. J. and Madireddy, S. and Wild, S. M. and Nord, B.},
	month = nov,
	year = {2021},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2021arXiv211100961C
Type: article},
	keywords = {Astrophysics - Astrophysics of Galaxies, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\ZSJT5M2M\\Ćiprijanović et al. - 2021 - Robustness of deep learning algorithms in astronom.pdf:application/pdf},
}

@article{lao_artificial_2021,
	title = {Artificial intelligence for celestial object census: the latest technology meets the oldest science},
	volume = {66},
	shorttitle = {Artificial intelligence for celestial object census},
	url = {https://ui.adsabs.harvard.edu/abs/2021SciBu..66.2145L},
	doi = {10.1016/j.scib.2021.07.015},
	abstract = {Large surveys using modern telescopes are producing images that are increasing exponentially in size and quality. Identifying objects in the generated images by visual recognition is time-consuming and labor-intensive, while classifying the extracted radio sources is even more challenging. To address these challenges, we develop a deep learning-based radio source detector, named {\textbackslash}textsc\{HeTu\}, which is capable of rapidly identifying and classifying radio sources in an automated manner for both compact and extended radio sources. {\textbackslash}textsc\{HeTu\} is based on a combination of a residual network (ResNet) and feature pyramid network (FPN). We classify radio sources into four classes based on their morphology. The training images are manually labeled and data augmentation methods are applied to solve the data imbalance between the different classes. {\textbackslash}textsc\{HeTu\} automatically locates the radio sources in the images and assigns them to one of the four classes. The experiment on the testing dataset shows an average operation time of 5.4 millisecond per image and a precision of 99.4{\textbackslash}\% for compact point-like sources and 98.1{\textbackslash}\% for double-lobe sources. We applied {\textbackslash}textsc\{HeTu\} to the images obtained from the GaLactic and the Galactic Extragalactic All-Object Murchison Wide-field Array (GLEAM) survey project. More than 96.9{\textbackslash}\% of the {\textbackslash}textsc\{HeTu\}-detected compact sources are matched compared to the source finding software used in the GLEAM. We also detected and classified 2,298 extended sources (including Fanaroff-Riley type I and II sources, and core-jet sources) above \$5{\textbackslash}sigma\$. The cross-matching rates of extended sources are higher than 97{\textbackslash}\%, showing excellent performance of {\textbackslash}textsc\{HeTu\} in identifying extended radio sources. {\textbackslash}textsc\{HeTu\} provides an efficient tool for radio source finding and classification and can be applied to other scientific fields.},
	urldate = {2022-11-14},
	journal = {Science Bulletin},
	author = {Lao, Baoqiang and An, Tao and Wang, Ailing and Xu, Zhijun and Guo, Shaoguang and Lv, Weijia and Wu, Xiaocong and Zhang, Yingkang},
	month = nov,
	year = {2021},
	note = {ADS Bibcode: 2021SciBu..66.2145L},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
	pages = {2145--2147},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\JXVNDWMA\\Lao et al. - 2021 - Artificial intelligence for celestial object censu.pdf:application/pdf},
}

@article{du_evolutionary_2021,
	title = {The {Evolutionary} {Pathways} of {Disk}-, {Bulge}-, and {Halo}-dominated {Galaxies}},
	volume = {919},
	issn = {0004-637X},
	url = {https://ui.adsabs.harvard.edu/abs/2021ApJ...919..135D},
	doi = {10.3847/1538-4357/ac0e98},
	abstract = {To break the degeneracy among galactic stellar components, we extract kinematic structures using the framework that was described in Du et al. For example, the concept of stellar halos is generalized to weakly rotating structures that are composed of loosely bound stars, which can hence be associated to both disk and elliptical type morphologies. By applying this method to central galaxies with stellar mass 1010-11.5 M⊙ from the TNG50 simulation, we identify three broadly-defined types of galaxies: galaxies dominated by disk, by bulge, or by stellar halo structures. We then use the simulation to infer the underlying connection between the growth of structures and physical processes over cosmic time. By tracing galaxies back in time, we recognize three fundamental regimes: an early phase of evolution (z ≳ 2), and internal and external (mainly mergers) processes that act at later times. We find that disk- and bulge-dominated galaxies are not significantly affected by mergers since z {\textasciitilde} 2. The difference in their present-day structures originates from two distinct evolutionary pathways-extended versus compact-that are likely to be determined by their parent dark matter halos (i.e., nature). In contrast, slow rotator elliptical galaxies are typically halo-dominated, forming by external processes (e.g., mergers) in the later phase (i.e., nurture). This picture challenges the general idea that elliptical galaxies are the same objects as classical bulges. In observations, both bulge- and halo-dominated galaxies are likely to be classified as early-type galaxies with compact morphology and quiescent star formation. However, here we find them to have very different evolutionary histories.},
	urldate = {2022-11-14},
	journal = {The Astrophysical Journal},
	author = {Du, Min and Ho, Luis C. and Debattista, Victor P. and Pillepich, Annalisa and Nelson, Dylan and Hernquist, Lars and Weinberger, Rainer},
	month = oct,
	year = {2021},
	note = {ADS Bibcode: 2021ApJ...919..135D},
	keywords = {1560, 1569, 578, 594, 595, 622, Astrophysics - Astrophysics of Galaxies, Galaxy bulges, Galaxy evolution, Galaxy formation, Galaxy structure, Spiral galaxies, Star formation},
	pages = {135},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\U4S3HXRH\\Du et al. - 2021 - The Evolutionary Pathways of Disk-, Bulge-, and Ha.pdf:application/pdf},
}

@article{ciprijanovic_deepmerge_2021,
	title = {{DeepMerge} - {II}. {Building} robust deep learning algorithms for merging galaxy identification across domains},
	volume = {506},
	issn = {0035-8711},
	url = {https://ui.adsabs.harvard.edu/abs/2021MNRAS.506..677C},
	doi = {10.1093/mnras/stab1677},
	abstract = {In astronomy, neural networks are often trained on simulation data with the prospect of being used on telescope observations. Unfortunately, training a model on simulation data and then applying it to instrument data leads to a substantial and potentially even detrimental decrease in model accuracy on the new target data set. Simulated and instrument data represent different data domains, and for an algorithm to work in both, domain-invariant learning is necessary. Here, we employ domain adaptation techniques - Maximum Mean Discrepancy as an additional transfer loss and Domain Adversarial Neural Networks - and demonstrate their viability to extract domain-invariant features within the astronomical context of classifying merging and non-merging galaxies. Additionally, we explore the use of Fisher loss and entropy minimization to enforce better in-domain class discriminability. We show that the addition of each domain adaptation technique improves the performance of a classifier when compared to conventional deep learning algorithms. We demonstrate this on two examples: between two Illustris-1 simulated data sets of distant merging galaxies, and between Illustris-1 simulated data of nearby merging galaxies and observed data from the Sloan Digital Sky Survey. The use of domain adaptation techniques in our experiments leads to an increase of target domain classification accuracy of up to \$\{{\textbackslash}sim \}20\{\{{\textbackslash} {\textbackslash}rm per{\textbackslash} cent\}\}\$. With further development, these techniques will allow astronomers to successfully implement neural network models trained on simulation data to efficiently detect and study astrophysical objects in current and future large-scale astronomical surveys.},
	urldate = {2022-11-14},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Ćiprijanović, A. and Kafkes, D. and Downey, K. and Jenkins, S. and Perdue, G. N. and Madireddy, S. and Johnston, T. and Snyder, G. F. and Nord, B.},
	month = sep,
	year = {2021},
	note = {ADS Bibcode: 2021MNRAS.506..677C},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, galaxies: evolution, galaxies: interactions, methods: data analysis, techniques: image processing},
	pages = {677--691},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\SZI7E6D5\\Ćiprijanović et al. - 2021 - DeepMerge - II. Building robust deep learning algo.pdf:application/pdf},
}

@techreport{coccomini_generative_2021,
	title = {Generative {Adversarial} {Networks} for {Astronomical} {Images} {Generation}},
	url = {https://ui.adsabs.harvard.edu/abs/2021arXiv211111578C},
	abstract = {Space exploration has always been a source of inspiration for humankind, and thanks to modern telescopes, it is now possible to observe celestial bodies far away from us. With a growing number of real and imaginary images of space available on the web and exploiting modern deep Learning architectures such as Generative Adversarial Networks, it is now possible to generate new representations of space. In this research, using a Lightweight GAN, a dataset of images obtained from the web, and the Galaxy Zoo Dataset, we have generated thousands of new images of celestial bodies, galaxies, and finally, by combining them, a wide view of the universe. The code for reproducing our results is publicly available at https://github.com/davide-coccomini/GAN-Universe, and the generated images can be explored at https://davide-coccomini.github.io/GAN-Universe/.},
	urldate = {2022-11-14},
	author = {Coccomini, Davide and Messina, Nicola and Gennaro, Claudio and Falchi, Fabrizio},
	month = nov,
	year = {2021},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2021arXiv211111578C
Type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\B6TL8J8H\\Coccomini et al. - 2021 - Generative Adversarial Networks for Astronomical I.pdf:application/pdf},
}

@article{stein_mining_2022,
	title = {Mining for {Strong} {Gravitational} {Lenses} with {Self}-supervised {Learning}},
	volume = {932},
	issn = {0004-637X},
	url = {https://ui.adsabs.harvard.edu/abs/2022ApJ...932..107S},
	doi = {10.3847/1538-4357/ac6d63},
	abstract = {We employ self-supervised representation learning to distill information from 76 million galaxy images from the Dark Energy Spectroscopic Instrument Legacy Imaging Surveys' Data Release 9. Targeting the identification of new strong gravitational lens candidates, we first create a rapid similarity search tool to discover new strong lenses given only a single labeled example. We then show how training a simple linear classifier on the self-supervised representations, requiring only a few minutes on a CPU, can automatically classify strong lenses with great efficiency. We present 1192 new strong lens candidates that we identified through a brief visual identification campaign and release an interactive web-based similarity search tool and the top network predictions to facilitate crowd-sourcing rapid discovery of additional strong gravitational lenses and other rare objects: github.com/georgestein/ssl-legacysurvey.},
	urldate = {2022-11-14},
	journal = {The Astrophysical Journal},
	author = {Stein, George and Blaum, Jacqueline and Harrington, Peter and Medan, Tomislav and Lukić, Zarija},
	month = jun,
	year = {2022},
	note = {ADS Bibcode: 2022ApJ...932..107S},
	keywords = {1464, 1643, 1938, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Computer Vision and Pattern Recognition, Convolutional neural networks, Sky surveys, Strong gravitational lensing},
	pages = {107},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\93J7UL6V\\Stein et al. - 2022 - Mining for Strong Gravitational Lenses with Self-s.pdf:application/pdf},
}

@techreport{ethiraj_classification_2022,
	title = {Classification of {Quasars}, {Galaxies}, and {Stars} in the {Mapping} of the {Universe} {Multi}-modal {Deep} {Learning}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv220510745E},
	abstract = {In this paper, the fourth version the Sloan Digital Sky Survey (SDSS-4), Data Release 16 dataset was used to classify the SDSS dataset into galaxies, stars, and quasars using machine learning and deep learning architectures. We efficiently utilize both image and metadata in tabular format to build a novel multi-modal architecture and achieve state-of-the-art results. In addition, our experiments on transfer learning using Imagenet weights on five different architectures (Resnet-50, DenseNet-121 VGG-16, Xception, and EfficientNet) reveal that freezing all layers and adding a final trainable layer may not be an optimal solution for transfer learning. It is hypothesized that higher the number of trainable layers, higher will be the training time and accuracy of predictions. It is also hypothesized that any subsequent increase in the number of training layers towards the base layers will not increase in accuracy as the pre trained lower layers only help in low level feature extraction which would be quite similar in all the datasets. Hence the ideal level of trainable layers needs to be identified for each model in respect to the number of parameters. For the tabular data, we compared classical machine learning algorithms (Logistic Regression, Random Forest, Decision Trees, Adaboost, LightGBM etc.,) with artificial neural networks. Our works shed new light on transfer learning and multi-modal deep learning architectures. The multi-modal architecture not only resulted in higher metrics (accuracy, precision, recall, F1 score) than models using only image data or tabular data. Furthermore, multi-modal architecture achieved the best metrics in lesser training epochs and improved the metrics on all classes.},
	urldate = {2022-11-14},
	author = {Ethiraj, Sabeesh and Bolla, Bharath Kumar},
	month = may,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv220510745E
Type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\QW95AY9F\\Ethiraj and Bolla - 2022 - Classification of Quasars, Galaxies, and Stars in .pdf:application/pdf},
}

@techreport{oreste_pinciroli_vago_deepgravilens_2022,
	title = {{DeepGraviLens}: a {Multi}-{Modal} {Architecture} for {Classifying} {Gravitational} {Lensing} {Data}},
	shorttitle = {{DeepGraviLens}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv220500701O},
	abstract = {Gravitational lensing is the relativistic effect generated by massive bodies, which bend the space-time surrounding them. It is a deeply investigated topic in astrophysics and allows validating theoretical relativistic results and studying faint astrophysical objects that would not be visible otherwise. In recent years Machine Learning methods have been applied to support the analysis of the gravitational lensing phenomena by detecting lensing effects in data sets consisting of images associated with brightness variation time series. However, the state-of-art approaches either consider only images and neglect time-series data or achieve relatively low accuracy on the most difficult data sets. This paper introduces DeepGraviLens, a novel multi-modal network that classifies spatio-temporal data belonging to one non-lensed system type and three lensed system types. It surpasses the current state of the art accuracy results by \${\textbackslash}approx\$ 19\% to \${\textbackslash}approx\$ 43\%, depending on the considered data set. Such an improvement will enable the acceleration of the analysis of lensed objects in upcoming astrophysical surveys, which will exploit the petabytes of data collected, e.g., from the Vera C. Rubin Observatory.},
	urldate = {2022-11-14},
	author = {Oreste Pinciroli Vago, Nicolò and Fraternali, Piero},
	month = may,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv220500701O
Type: article},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, General Relativity and Quantum Cosmology},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\I6H8RY85\\Oreste Pinciroli Vago and Fraternali - 2022 - DeepGraviLens a Multi-Modal Architecture for Clas.pdf:application/pdf},
}

@article{vazquez-mata_sdss_2022,
	title = {{SDSS} {IV} {MaNGA}: visual morphological and statistical characterization of the {DR15} sample},
	volume = {512},
	issn = {0035-8711},
	shorttitle = {{SDSS} {IV} {MaNGA}},
	url = {https://ui.adsabs.harvard.edu/abs/2022MNRAS.512.2222V},
	doi = {10.1093/mnras/stac635},
	abstract = {We present a detailed visual morphological classification for the 4614 MaNGA galaxies in SDSS Data Release 15, using image mosaics generated from a combination of r band (SDSS and deeper DESI Legacy Surveys) images and their digital post-processing. We distinguish 13 Hubble types and identify the presence of bars and bright tidal debris. After correcting the MaNGA sample for volume completeness, we calculate the morphological fractions, the bi-variate distribution of type and stellar mass M* - where we recognize a morphological transition 'valley' around S0a-Sa types - and the variations of the g - i colour and luminosity-weighted age over this distribution. We identified bars in 46.8 per cent of galaxies, present in all Hubble types later than S0. This fraction amounts to a factor {\textasciitilde}2 larger when compared with other works for samples in common. We detected 14 per cent of galaxies with tidal features, with the fraction changing with M* and morphology. For 355 galaxies, the classification was uncertain; they are visually faint, mostly of low/intermediate masses, low concentrations, and discy in nature. Our morphological classification agrees well with other works for samples in common, though some particular differences emerge, showing that our image procedures allow us to identify a wealth of added value information as compared to SDSS-based previous estimates. Based on our classification, we also propose an alternative criteria for the E-S0 separation, in the structural semimajor to semiminor axis versus bulge to total light ratio (b/a - B/T) and concentration versus semimajor to semiminor axis (C - b/a) space.},
	urldate = {2022-11-14},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Vázquez-Mata, J. A. and Hernández-Toledo, H. M. and Avila-Reese, V. and Herrera-Endoqui, M. and Rodríguez-Puebla, A. and Cano-Díaz, M. and Lacerna, I. and Martínez-Vázquez, L. A. and Lane, R.},
	month = may,
	year = {2022},
	note = {ADS Bibcode: 2022MNRAS.512.2222V},
	keywords = {Astrophysics - Astrophysics of Galaxies, catalogues, galaxies: fundamental parameters, galaxies: structure},
	pages = {2222--2244},
	file = {Submitted Version:C\:\\Users\\annab\\Zotero\\storage\\YEFMTB9M\\Vázquez-Mata et al. - 2022 - SDSS IV MaNGA visual morphological and statistica.pdf:application/pdf},
}

@techreport{poli_self-similarity_2022,
	title = {Self-{Similarity} {Priors}: {Neural} {Collages} as {Differentiable} {Fractal} {Representations}},
	shorttitle = {Self-{Similarity} {Priors}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv220407673P},
	abstract = {Many patterns in nature exhibit self-similarity: they can be compactly described via self-referential transformations. Said patterns commonly appear in natural and artificial objects, such as molecules, shorelines, galaxies and even images. In this work, we investigate the role of learning in the automated discovery of self-similarity and in its utilization for downstream tasks. To this end, we design a novel class of implicit operators, Neural Collages, which (1) represent data as the parameters of a self-referential, structured transformation, and (2) employ hypernetworks to amortize the cost of finding these parameters to a single forward pass. We investigate how to leverage the representations produced by Neural Collages in various tasks, including data compression and generation. Neural Collages image compressors are orders of magnitude faster than other self-similarity-based algorithms during encoding and offer compression rates competitive with implicit methods. Finally, we showcase applications of Neural Collages for fractal art and as deep generative models.},
	urldate = {2022-11-14},
	author = {Poli, Michael and Xu, Winnie and Massaroli, Stefano and Meng, Chenlin and Kim, Kuno and Ermon, Stefano},
	month = apr,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv220407673P
Type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\56NEHZW6\\Poli et al. - 2022 - Self-Similarity Priors Neural Collages as Differe.pdf:application/pdf},
}

@article{gharat_galaxy_2022,
	title = {Galaxy classification: a deep learning approach for classifying {Sloan} {Digital} {Sky} {Survey} images},
	volume = {511},
	issn = {0035-8711},
	shorttitle = {Galaxy classification},
	url = {https://ui.adsabs.harvard.edu/abs/2022MNRAS.511.5120G},
	doi = {10.1093/mnras/stac457},
	abstract = {In recent decades, large-scale sky surveys such as Sloan Digital Sky Survey (SDSS) have resulted in generation of tremendous amount of data. The classification of this enormous amount of data by astronomers is time consuming. To simplify this process, in 2007 a volunteer-based citizen science project called 'Galaxy Zoo' was introduced, which has reduced the time for classification by a good extent. However, in this modern era of deep learning, automating this classification task is highly beneficial as it reduces the time for classification. For the last few years, many algorithms have been proposed which happen to do a phenomenal job in classifying galaxies into multiple classes. But all these algorithms tend to classify galaxies into less than six classes. However, after considering the minute information which we know about galaxies, it is necessary to classify galaxies into more than eight classes. In this study, a neural network model is proposed so as to classify SDSS data into 10 classes from an extended Hubble Tuning Fork. Great care is given to disc edge and disc face galaxies, distinguishing between a variety of substructures and minute features which are associated with each class. The proposed model consists of convolution layers to extract features making this method fully automatic. The achieved test accuracy is 84.73 per cent which happens to be promising after considering such minute details in classes. Along with convolution layers, the proposed model has three more layers responsible for classification, which makes the algorithm consume less time.},
	urldate = {2022-11-14},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Gharat, Sarvesh and Dandawate, Yogesh},
	month = apr,
	year = {2022},
	note = {ADS Bibcode: 2022MNRAS.511.5120G},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Computer Vision and Pattern Recognition, galaxies: general, Galaxy: structure, methods: miscellaneous, techniques: image processing},
	pages = {5120--5124},
	file = {Submitted Version:C\:\\Users\\annab\\Zotero\\storage\\VLEAYS9J\\Gharat and Dandawate - 2022 - Galaxy classification a deep learning approach fo.pdf:application/pdf},
}

@article{walmsley_galaxy_2022,
	title = {Galaxy {Zoo} {DECaLS}: {Detailed} visual morphology measurements from volunteers and deep learning for 314 000 galaxies},
	volume = {509},
	issn = {0035-8711},
	shorttitle = {Galaxy {Zoo} {DECaLS}},
	url = {https://ui.adsabs.harvard.edu/abs/2022MNRAS.509.3966W},
	doi = {10.1093/mnras/stab2093},
	abstract = {We present Galaxy Zoo DECaLS: detailed visual morphological classifications for Dark Energy Camera Legacy Survey images of galaxies within the SDSS DR8 footprint. Deeper DECaLS images (r = 23.6 versus r = 22.2 from SDSS) reveal spiral arms, weak bars, and tidal features not previously visible in SDSS imaging. To best exploit the greater depth of DECaLS images, volunteers select from a new set of answers designed to improve our sensitivity to mergers and bars. Galaxy Zoo volunteers provide 7.5 million individual classifications over 314 000 galaxies. 140 000 galaxies receive at least 30 classifications, sufficient to accurately measure detailed morphology like bars, and the remainder receive approximately 5. All classifications are used to train an ensemble of Bayesian convolutional neural networks (a state-of-the-art deep learning method) to predict posteriors for the detailed morphology of all 314 000 galaxies. We use active learning to focus our volunteer effort on the galaxies which, if labelled, would be most informative for training our ensemble. When measured against confident volunteer classifications, the trained networks are approximately 99 per cent accurate on every question. Morphology is a fundamental feature of every galaxy; our human and machine classifications are an accurate and detailed resource for understanding how galaxies evolve.},
	urldate = {2022-11-14},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Walmsley, Mike and Lintott, Chris and Géron, Tobias and Kruk, Sandor and Krawczyk, Coleman and Willett, Kyle W. and Bamford, Steven and Kelvin, Lee S. and Fortson, Lucy and Gal, Yarin and Keel, William and Masters, Karen L. and Mehta, Vihang and Simmons, Brooke D. and Smethurst, Rebecca and Smith, Lewis and Baeten, Elisabeth M. and Macmillan, Christine},
	month = jan,
	year = {2022},
	note = {ADS Bibcode: 2022MNRAS.509.3966W},
	keywords = {Astrophysics - Astrophysics of Galaxies, Computer Science - Computer Vision and Pattern Recognition, galaxies: bar, galaxies: general, galaxies: interactions, methods: data analysis},
	pages = {3966--3988},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\GVCRLF54\\Walmsley et al. - 2022 - Galaxy Zoo DECaLS Detailed visual morphology meas.pdf:application/pdf},
}
